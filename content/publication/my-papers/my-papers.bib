
@inproceedings{yuta_nakashima_estimation_2006,
	address = {Geneva, Switzerland},
	title = {Estimation of recording location using audio watermarking},
	isbn = {978-1-59593-493-2},
	url = {http://portal.acm.org/citation.cfm?doid=1161366.1161385},
	doi = {10.1145/1161366.1161385},
	abstract = {In this paper, we propose a novel application of audio watermarking, estimation of recording location. The purpose of the paper is to determine the seat location in a theater at which a bootleg recording was made by using a digital video camera. In the proposed application, we embed different watermarks in the channels of the multi-channel sound of the movie. The multi-channel sound enters the air from multiple loudspeakers in a theater. If a monaural recording of the sound is made, the location of recording in the theater can be determined by detecting the multiple watermarks without any access to the host signals. The main idea is to use the time offsets of the watermark signals in the recorded signal. In simulation experiments, the recording locations can be determined within 1 m in almost all of the test cases. Although an experiment in a real environment resulted in more errors due to noise, these results successfully showed the potential applicability for practical use.},
	language = {en},
	urldate = {2019-04-29},
	booktitle = {Proc. 8th {Workshop} on {Multimedia} and {Security} ({MM}\&{Sec})},
	publisher = {ACM Press},
	author = {{Yuta Nakashima} and {Ryuki Tachibana} and {Masafumi Nishimura} and {Noboru Babaguchi}},
	month = jun,
	year = {2006},
	pages = {108--113}
}

@techreport{___2006,
	title = {音楽に対する電子透かしを用いた位置推定},
	url = {https://jglobal.jst.go.jp/detail?JGLOBAL_ID=200902278335628195},
	language = {ja},
	number = {日本音響学会2006年秋季研究発表会, 2-1-9},
	urldate = {2019-04-29},
	author = {{中島 悠太} and {立花 隆輝} and {西村 雅史} and {馬場口 登}},
	month = sep,
	year = {2006},
	pages = {458--459},
	file = {Snapshot:files/4/detail.html:text/html}
}

@inproceedings{nakashima_determining_2007,
	address = {Honolulu, HI},
	title = {Determining recording location based on synchronization positions of audio watermarking},
	isbn = {978-1-4244-0727-9},
	url = {http://ieeexplore.ieee.org/document/4217393/},
	doi = {10.1109/ICASSP.2007.366220},
	abstract = {In this paper, we propose a novel application of digital watermarking, determination of recording locations. This application enables us to determine the seat location in an auditorium where a recording was made. Precisely measured synchronization positions of the spread-spectrum watermarks are used for the determination. To avoid use of mismeasured synchronization positions, the algorithm discards synchronization positions with the corresponding normalized correlation values below a threshold. The experiments with our implementation resulted in accurate determinations; almost all of the locations can be determined within the error of 0.5 m. These experimental results successfully show the potential applicability of our application.},
	urldate = {2019-04-29},
	booktitle = {Proc. {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Nakashima, Yuta and Tachibana, Ryuki and Nishimura, Masafumi and Babaguchi, Noboru},
	month = apr,
	year = {2007},
	pages = {II--253--II--256},
	file = {Nakashima et al. - 2007 - Determining recording location based on synchroniz.pdf:files/29/Nakashima et al. - 2007 - Determining recording location based on synchroniz.pdf:application/pdf}
}

@techreport{___2007,
	title = {音楽に対する電子透かしの同期位置に基づく録音位置の最尤推定},
	number = {電子情報通信学会 2007年マルチメディア情報ハイディング研究会, MIH02-09},
	author = {{中島 悠太} and {兼頭 亮介} and {立花 隆輝} and {馬場口 登}},
	month = jul,
	year = {2007},
	pages = {6}
}

@inproceedings{nakashima_maximum-likelihood_2007,
	address = {Kaohsiung, Taiwan},
	title = {Maximum-likelihood estimation of recording position based on audio watermarking},
	url = {http://ieeexplore.ieee.org/document/4457699/},
	doi = {10.1109/IIH-MSP.2007.221},
	urldate = {2019-04-29},
	booktitle = {Proc. {Third} {International} {Conference} on {Intelligent} {Information} {Hiding} and {Multimedia} {Signal} {Processing} ({IIHMSP})},
	publisher = {IEEE},
	author = {Nakashima, Yuta and Tachibana, Ryuki and Babaguchi, Noboru},
	month = nov,
	year = {2007},
	pages = {255--258},
	file = {Nakashima et al. - 2007 - Maximum-likelihood estimation of recording positio.pdf:files/28/Nakashima et al. - 2007 - Maximum-likelihood estimation of recording positio.pdf:application/pdf}
}

@article{nakashima_watermarked_2009,
	title = {Watermarked movie soundtrack finds the position of the camcorder in a theater},
	volume = {11},
	issn = {1520-9210},
	doi = {10.1109/TMM.2009.2012938},
	abstract = {In recent years, the problem of camcorder piracy in theaters has become more serious due to technical advances in camcorders. In this paper, as a new deterrent to camcorder piracy, we propose a system for estimating the recording position from which a camcorder recording is made. The system is based on spread-spectrum audio watermarking for the multichannel movie soundtrack. It utilizes a stochastic model of the detection strength, which is calculated in the watermark detection process. Our experimental results show that the system estimates recording positions in an actual theater with a mean estimation error of 0.44 m. The results of our MUSHRA subjective listening tests show the method does not significantly spoil the subjective acoustic quality of the soundtrack. These results indicate that the proposed system is applicable for practical uses.},
	number = {3},
	journal = {IEEE Transactions on Multimedia},
	author = {Nakashima, Yuta and Tachibana, Ryuki and Babaguchi, Noboru},
	month = apr,
	year = {2009},
	keywords = {acoustic quality, acoustic signal detection, Acoustic signal detection, Acoustic testing, audio coding, Audio recording, Audio watermarking, camcorder piracy, cinema theater, cinematography, computer crime, Estimation error, Internet, Motion pictures, movie soundtrack, movie soundtrack watermarking, multichannel movie soundtrack, position estimation, prevention of movie piracy, recording position estimation, spread spectrum communication, Spread spectrum communication, spread-spectrum audio watermarking, stochastic model, stochastic processes, Stochastic processes, video cameras, Video equipment, video surveillance, watermark detection process, watermarking, Watermarking},
	pages = {443--454},
	file = {IEEE Xplore Abstract Record:files/9/4781786.html:text/html}
}

@techreport{___2009,
	title = {音響電子透かしの検出強度を用いた位置推定},
	url = {http://www.ieice.org/iss/mih/workshop/mih090317.html},
	number = {電子情報通信学会 2009年マルチメディア情報ハイディング研究会, DS-3-10},
	author = {{兼頭 亮介} and {中島 悠太} and {馬場口 登}},
	month = mar,
	year = {2009},
	pages = {2}
}

@techreport{___2009-1,
	title = {映像特徴に基づく撮影者が意図した人物被写体の推定},
	url = {https://ci.nii.ac.jp/naid/110008100722},
	urldate = {2019-04-29},
	author = {{上柿 普史} and {中島 悠太} and {馬場口 登}},
	month = aug,
	year = {2009},
	pages = {639--642},
	file = {K-046 映像特徴に基づく撮影者が意図した人物被写体の推定(教育工学・福祉工学・マルチメディア応用,一般論文) Snapshot:files/12/110008100722.html:text/html}
}

@inproceedings{takehara_digital_2009,
	title = {Digital {Diorama}: {Real}-time adaptive visualization of public spaces},
	booktitle = {Proc. 1st {International} {Conference} on {Security} {Camera} {Network}, {Privacy} {Protection} and {Community} {Safety} ({SPC})},
	author = {Takehara, Takumi and Nakashima, Yuta and Nitta, Naoko and Babaguchi, Noboru},
	month = oct,
	year = {2009},
	pages = {2}
}

@techreport{___2010,
	title = {映像中の撮影者が意図した人物被写体の検出},
	url = {https://jglobal.jst.go.jp/detail?JGLOBAL_ID=201002288266281966},
	language = {ja},
	number = {電子情報通信学会2010年総合大会, D-12-41},
	urldate = {2019-04-29},
	author = {{中島 悠太} and {上柿 普史} and {馬場口 登}},
	month = mar,
	year = {2010},
	pages = {152--152},
	file = {Snapshot:files/15/detail.html:text/html}
}

@techreport{___2010-1,
	title = {音響電子透かしを用いた屋内での録音位置推定},
	url = {https://jglobal.jst.go.jp/detail?JGLOBAL_ID=201002249884600198},
	abstract = {本研究では,音響電子透かしを用いた位置推定手法を提案した。実験から,用いる音楽や音量により,1.0m以内のRMSEで静止位置推定が可能であることが明らかになり、移動する場合も位置推定可能であることが示唆された。これにより、大規模集客施設でのナビゲーションなどへの応用が可能であると考えられる。今後は、体での遮蔽がある場合でも正確な検出強度列のピーク値のモデルの構築や、より頑健な電子透かし手法の提案などにより、位置推定精度向上に取り組みたい。},
	language = {ja},
	number = {電子情報通信学会 マルチメディア情報ハイディング研究会, DS-3-1},
	urldate = {2019-04-29},
	author = {{兼頭 亮介} and {中島 悠太} and {馬場口 登}},
	month = mar,
	year = {2010},
	pages = {15--16},
	file = {Snapshot:files/17/detail.html:text/html}
}

@inproceedings{nakashima_detecting_2010,
	title = {Detecting intended human objects in human-captured videos},
	doi = {10.1109/CVPRW.2010.5543721},
	abstract = {When people take videos, they always want to capture intended objects, which are essential for presenting what they want to express in their videos, and to share the intended objects with others. The concept of intended objects provide a novel perspective for video content analysis, and detecting intended objects may be beneficial for wide range of applications such as video understanding and semantics interpretation, video summarization, video adaptation, video privacy protection, and so on. In this paper, we focus on a particular type of intended objects, i.e., intended human objects, and an interesting method is developed for detecting intended human objects automatically from human-captured videos. We also investigate the correlation between intended human objects and visual attention. Our experimental results indicate that our method can successfully detect the intended human objects.},
	booktitle = {Proc. 2010 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops}},
	author = {Nakashima, Yuta and Babaguchi, Noboru and Fan, Jianping},
	month = jun,
	year = {2010},
	keywords = {Cameras, Computer science, Displays, Face, human captured video, human object detection, Humans, object detection, Object detection, Pediatrics, Privacy, Protection, semantics interpretation, video adaptation, video content analysis, Video equipment, video privacy protection, video recording, video summarization},
	pages = {33--40},
	file = {IEEE Xplore Abstract Record:files/19/5543721.html:text/html;Nakashima et al. - 2010 - Detecting intended human objects in human-captured.pdf:files/21/Nakashima et al. - 2010 - Detecting intended human objects in human-captured.pdf:application/pdf}
}

@inproceedings{takehara_digital_2010,
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Digital {Diorama}: {Sensing}-based real-world visualization},
	isbn = {978-3-642-14058-7},
	shorttitle = {Digital {Diorama}},
	abstract = {Many sensors around the world are consistently collecting the real-time real-world data. The data streams captured by these sensors can give us an idea of what is going on in a specific area; however, it is not easy for humans to understand their spatial and temporal relationships by just looking at them independently. This paper proposes to construct Digital Diorama, a three-dimensional view where viewers can see at a glance how people are moving around the monitored space without violating their privacy, by integrating multiple data streams captured by stationary cameras and RFID readers in real time. Digital Diorama realizes such real-world visualization with the following features: 1) view control, 2) real-time camera image superimposition, and 3) privacy control. We have demonstrated that Digital Diorama for a shopping center was able to present the current positions of persons and real-time camera images in approximately 1 frame per second.},
	language = {en},
	booktitle = {Proc. {International} {Conference} on {Information} {Processing} and {Management} of {Uncertainty} in {Knowledge}-{Based} {Systems} ({IPMU})},
	publisher = {Springer Berlin Heidelberg},
	author = {Takehara, Takumi and Nakashima, Yuta and Nitta, Naoko and Babaguchi, Noboru},
	editor = {Hüllermeier, Eyke and Kruse, Rudolf and Hoffmann, Frank},
	month = jun,
	year = {2010},
	keywords = {Camera Image, Camera Position, Privacy Information, Shopping Center, Stationary Camera},
	pages = {663--672}
}

@inproceedings{uegaki_discriminating_2010,
	title = {Discriminating intended human objects in consumer videos},
	doi = {10.1109/ICPR.2010.1065},
	abstract = {In a consumer video, there are not only intended objects, which are intentionally captured by the camcorder user, but also unintended objects, which are accidentally framed-in. Since the intended objects are essential to present what the camcorder user wants to express in the video, discriminating the intended objects from the unintended objects are beneficial for many applications, e.g., video summarization, privacy protection, and so forth. In this paper, focusing on human objects, we propose a method for discriminating the intended human objects from the unintended human objects. We evaluated the proposed method using 10 videos captured by 3 camcorder users. The results demonstrate that the proposed method successfully discriminates the intended human objects with 0.45 of recall and 0.80 of precision.},
	booktitle = {Proc. 20th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Uegaki, Hiroshi and Nakashima, Yuta and Babaguchi, Noboru},
	month = aug,
	year = {2010},
	keywords = {camcorder, consumer video, Face, Hidden Markov models, Humans, Image color analysis, intended human object, intended human object discrimination, intended object, object detection, Skin, unintended human object, Video equipment, video signal processing, Videos},
	pages = {4380--4383},
	file = {IEEE Xplore Abstract Record:files/23/5597875.html:text/html}
}

@inproceedings{kaneto_real-time_2010,
	title = {Real-time user position estimation in indoor environments using digital watermarking for audio signals},
	doi = {10.1109/ICPR.2010.32},
	abstract = {In this paper, we propose a method for estimating the user position where a user is holding a microphone in an indoor environment using digital watermarking for audio signals. The proposed method utilizes detection strengths, which are calculated while detecting spread-spectrum-based watermarks. Taking into account delays and attenuation of the watermarked signals emitted from multiple loudspeakers and other factors, we construct a model of detection strengths. The user position is estimated in real-time using the model. The experimental results indicate that the user positions are estimated with 1.3 m of root mean squared error on average for the case where the user is static. We demonstrate that the proposed method successfully estimates the user position even when the user moves.},
	booktitle = {Proc. 20th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Kaneto, Ryosuke and Nakashima, Yuta and Babaguchi, Noboru},
	month = aug,
	year = {2010},
	keywords = {Accuracy, Atmospheric modeling, audio coding, audio signal, digital watermarking, digital watermarking for audio signals, Estimation, indoor environment, Loudspeakers, microphone, Microphones, Real time systems, Real-time position estimation, real-time systems, real-time user position estimation, root mean square error, spread-spectrum-based watermark, watermarking, Watermarking},
	pages = {97--100},
	file = {IEEE Xplore Abstract Record:files/25/5597637.html:text/html}
}

@inproceedings{nakashima_automatically_2010,
	address = {New York, NY, USA},
	series = {{MM} '10},
	title = {Automatically {Protecting} {Privacy} in {Consumer} {Generated} {Videos} {Using} {Intended} {Human} {Object} {Detector}},
	isbn = {978-1-60558-933-6},
	url = {http://doi.acm.org/10.1145/1873951.1874169},
	doi = {10.1145/1873951.1874169},
	abstract = {The growing popularity of video sharing services such as YouTube enables us to upload and share consumer generated videos (CGVs) easily, resulting in disclosure of the privacy sensitive information (PSI) of persons, i.e., their appearances. Therefore, we need a technique for automatically protecting the privacy in CGVs; however, the main problem is how to determine PSI regions automatically. In this paper, we propose a novel system for automatically protecting the privacy in CGVs. The proposed system tackles the problem of determining PSI regions by using an intended human object detector that detects human objects which the camera person wanted to capture to achieve his/her capture intention. In addition, the proposed system adopts several PSI obscuring methods such as blocking out, blurring and seam carving. We present the results of subjective evaluations of a privacy protected video in terms of the visual quality and acceptability of PSI disclosure, as well as the performance of the intended human object detector.},
	urldate = {2019-04-29},
	booktitle = {Proc. 18th {ACM} {International} {Conference} on {Multimedia} ({ACM} {MM})},
	publisher = {ACM},
	author = {Nakashima, Yuta and Babaguchi, Noboru and Fan, Jianping},
	month = oct,
	year = {2010},
	note = {event-place: Firenze, Italy},
	keywords = {consumer generated videos, intended human object detector, privacy protection},
	pages = {1135--1138},
	file = {Nakashima et al. - 2010 - Automatically Protecting Privacy in Consumer Gener.pdf:files/27/Nakashima et al. - 2010 - Automatically Protecting Privacy in Consumer Gener.pdf:application/pdf}
}

@inproceedings{nakashima_automatic_2011,
	title = {Automatic generation of privacy-protected videos using background estimation},
	doi = {10.1109/ICME.2011.6011955},
	abstract = {Recently, video sharing services such as YouTube and Daily-motion have become popular and many videos taken with mobile video cameras are uploaded to such a video sharing service. However, such videos can infringe on the privacy right of people in the videos because they may contain privacy sensitive information (PSI) of the people, i.e., their appearances. This strongly motivates us to develop a technique to generate privacy-protected videos. In this paper, we propose a novel system for automatic generation of privacy-protected videos based on background estimation. In most conventional techniques, objects that contain PSI are detected and obscured by, e.g., blurring. Conversely, in our system, background pixels are estimated and then substituted with intended human objects that are essential for the camera person's capture intention. We quantitatively evaluate our system to demonstrate its potential applicability.},
	booktitle = {Proc. 2011 {IEEE} {International} {Conference} on {Multimedia} and {Expo} ({ICME})},
	author = {Nakashima, Yuta and Babaguchi, Noboru and Fan, Jianping},
	month = jul,
	year = {2011},
	keywords = {background estimation, graph cuts, Humans, intended human object detection, OWL, Privacy, Privacy protection for video},
	pages = {6},
	file = {IEEE Xplore Abstract Record:files/31/6011955.html:text/html;Nakashima et al. - 2011 - Automatic generation of privacy-protected videos u.pdf:files/32/Nakashima et al. - 2011 - Automatic generation of privacy-protected videos u.pdf:application/pdf}
}

@techreport{___2011,
	title = {カメラの動きと映像特徴からの撮影者が意図した領域の推定},
	url = {https://ipsj.ixsq.nii.ac.jp/ej/index.php?active_action=repository_view_main_item_detail&page_id=13&block_id=8&item_id=77877&item_no=1},
	abstract = {映像の要約や編集などのアプリケーションでは，重要な領域を推定し，その領域に基づいて映像を処理す ることから，重要な領域の推定手法が必要とされている.一方，YouTube などにはモバイルカメラで撮影された映像 が多く投稿されており，これらの映像には「自分の子供を撮影したい」などの撮影意図が存在する.このとき「子供 の領域」のような撮影者が意図的に撮影した領域(意図領域)は撮影意図に不可欠である.本稿では，意図領域を重 要な領域として推定する手法について述べる.提案手法では，撮影者はフレーム中で意図領域を適切に配置するよう にカメラを動かすことから，撮影者の行動が反映されるカメラの動きと映像特徴のモデルを構築し，このモデルに基 づいて意図領域を推定する.さらに，実験により提案手法の有効性を示す.},
	language = {ja},
	number = {画像の認識・理解シンポジウム（MIRU2011）論文集},
	urldate = {2019-04-29},
	author = {{上柿 普史} and {中島 悠太} and {馬場口 登}},
	month = jul,
	year = {2011},
	pages = {1645--1652},
	file = {Full Text PDF:files/34/上柿普史 et al. - 2011 - カメラの動きと映像特徴からの撮影者が意図した領域の推定.pdf:application/pdf;IPSJ-MIRU2011237.pdf:files/36/IPSJ-MIRU2011237.pdf:application/pdf;Snapshot:files/35/ej.html:text/html}
}

@article{nakashima_indoor_2011,
	title = {Indoor positioning system using digital audio watermarking},
	volume = {E94.D},
	issn = {0916-8532, 1745-1361},
	url = {https://www.jstage.jst.go.jp/article/transinf/E94.D/11/E94.D_11_2201/_article},
	doi = {10.1587/transinf.E94.D.2201},
	abstract = {Recently, a number of location-based services such as navigation and mobile advertising have been proposed. Such services require real-time user positions. Since a global positioning system (GPS), which is one of the most well-known techniques for real-time positioning, is unsuitable for indoor uses due to unavailability of GPS signals, many indoor positioning systems (IPSs) using WLAN, radio frequency identification tags, and so forth have been proposed. However, most of them suffer from high installation costs. In this paper, we propose a novel IPS for real-time positioning that utilizes a digital audio watermarking technique. The proposed IPS first embeds watermarks into an audio signal to generate watermarked signals, each of which is then emitted from a corresponding speaker installed in a target environment. A user of the proposed IPS receives the watermarked signals with a mobile device equipped with a microphone, and the watermarks are detected in the received signal. For positioning, we model various effects upon watermarks due to propagation in the air, i.e., delays, attenuation, and diffraction. The model enables the proposed IPS to accurately locate the user based on the watermarks detected in the received signal. The proposed IPS can be easily deployed with a low installation cost because the IPS can work with off-the-shelf speakers that have been already installed in most of the indoor environments such as department stores, amusement arcades, and airports. We experimentally evaluate the accuracy of positioning and show that the proposed IPS locates the user in a 6m by 7.5m room with root mean squared error of 2.25m on average. The results also demonstrate the potential capability of real-time positioning with the proposed IPS.},
	language = {en},
	number = {11},
	urldate = {2019-04-29},
	journal = {IEICE Transactions on Information and Systems},
	author = {Nakashima, Yuta and Kaneto, Ryosuke and Babaguchi, Noboru},
	month = nov,
	year = {2011},
	pages = {2201--2211},
	file = {Full Text PDF:files/38/Nakashima et al. - 2011 - Indoor Positioning System Using Digital Audio Wate.pdf:application/pdf;Snapshot:files/39/_article.html:text/html}
}

@inproceedings{nakashima_extracting_2011,
	address = {New York, NY, USA},
	series = {{MM} '11},
	title = {Extracting intentionally captured regions using point trajectories},
	isbn = {978-1-4503-0616-4},
	url = {http://doi.acm.org/10.1145/2072298.2072029},
	doi = {10.1145/2072298.2072029},
	abstract = {When camera persons take videos with mobile video cameras, they usually have capture intentions, i.e., what they want to express in their videos, and there are intentionally captured regions (ICRs) in the video frames that are essential for the capture intentions. Extracting ICRs is thus beneficial for wide range of applications such as video summarization and video adaptation for small displays. In this paper, we present a novel method for automatically extracting ICRs. A camera person usually moves his/her camera so that ICRs can be arranged in appropriate positions in video frames; therefore, ICRs can yield specific motion. This observation indicates that such specific motion is a vital cue for extracting ICRs. The proposed method represents motion by point trajectories, which are long-term trajectories of spatially dense points in video frames, and extracts ICRs using an ICR model based on the point trajectories. We experimentally evaluate the proposed method to demonstrate its potential applicability.},
	urldate = {2019-04-29},
	booktitle = {Proc. 19th {ACM} {International} {Conference} on {Multimedia} ({ACM} {MM})},
	publisher = {ACM},
	author = {Nakashima, Yuta and Babaguchi, Noboru},
	month = nov,
	year = {2011},
	note = {event-place: Scottsdale, Arizona, USA},
	keywords = {capture intention, intentionally captured region, point trajectory},
	pages = {1417--1420},
	file = {Nakashima and Babaguchi - 2011 - Extracting intentionally captured regions using po.pdf:files/41/Nakashima and Babaguchi - 2011 - Extracting intentionally captured regions using po.pdf:application/pdf}
}

@article{nakashima_intended_2012,
	title = {Intended human object detection for automatically protecting privacy in mobile video surveillance},
	volume = {18},
	issn = {1432-1882},
	url = {https://doi.org/10.1007/s00530-011-0244-y},
	doi = {10.1007/s00530-011-0244-y},
	abstract = {With the recent popularization of mobile video cameras including camera phones, a new technology, mobile video surveillance, which uses mobile video cameras for video surveillance has been emerging. Such videos, however, may infringe upon the privacy of others by disclosing privacy sensitive information (PSI), i.e., their appearances. To prevent videos from infringing on the right to privacy, new techniques are required that automatically obscure PSI regions. The problem is how to determine the PSI regions to be obscured while maintaining enough video content to present the camera persons’ capture-intentions, i.e., what they want to record in their videos to achieve their surveillance tasks. To this end, we introduce a new concept called intended human objects that are defined as human objects essential for capture-intentions, and develop a new method called intended human object detection that automatically detects the intended human objects in videos taken by different camera persons. Through the process of intended human object detection, we develop a system for automatically obscuring PSI regions. We experimentally show the performance of intended human object detection and the contributions of the features used. Our user study shows the potential applicability of our proposed system.},
	language = {en},
	number = {2},
	urldate = {2019-04-29},
	journal = {Multimedia Systems},
	author = {Nakashima, Yuta and Babaguchi, Noboru and Fan, Jianping},
	month = mar,
	year = {2012},
	keywords = {Intended human object detection, Mobile video camera, Mobile video surveillance, Privacy protection},
	pages = {157--173},
	file = {Nakashima et al. - 2012 - Intended human object detection for automatically .pdf:files/43/Nakashima et al. - 2012 - Intended human object detection for automatically .pdf:application/pdf}
}

@techreport{___2012,
	title = {顔画像に対するプライバシー保護処理の有効性の定量的評価},
	url = {https://ci.nii.ac.jp/naid/110009626162/},
	abstract = {画像や映像には人物の顔などのプライバシーセンシティブな情報(Privacy Sensitive Information: PSI)が含まれる可能性があるため，画像や映像を公開する際には，PSIを含む領域に対して，ぼかしや塗りつぶしなどの画像処理により視覚的抽象化を施すことでプライバシー侵害のリスクを低減する．しかし，これらの画像処理がどの程度プライバシー保護に有効であるかは明らかにされていない．そこで本報告では，プライバシー保護の有効性を定量的に評価する指標としてID可到達性を提案し，アンケート調査によりさまざまな画像処理のプライバシー保護に対する有効性を明らかにする．さらに，ぼかしなどについては，一定のID可到達性を達成するために必要なパラメータの導出を可能にする．},
	number = {電子情報通信学会 技術研究報告, EMM2012-9},
	urldate = {2019-04-29},
	author = {{中島 悠太} and {池野 知顕} and {馬場口 登}},
	month = jul,
	year = {2012},
	keywords = {ID可到達性, プライバシー保護, 定量的評価, 画像処理, 視覚的抽象化},
	pages = {59--66},
	file = {顔画像に対するプライバシー保護処理の有効性の定量的評価 Snapshot:files/45/110009626162.html:text/html}
}

@inproceedings{koyama_markov_2012,
	title = {Markov random field-based real-time detection of intentionally-captured persons},
	doi = {10.1109/ICIP.2012.6467125},
	abstract = {Most videos taken by videographers contain intentionally-captured persons (ICPs), who are essential for what the videographers want to express in their video. This paper presents a method to detect ICPs in real-time. Whether a person in a video is an ICP or not is reflected in features such as the person's motion and camera motion, which are thus beneficial for detecting ICPs. However, estimating camera motion is computationally expensive. For real-time detection, we use samples of acceleration and angular velocity obtained from inertial sensors instead of estimating camera motion. Considering that pairwise constraints based on differences between persons' sizes also improve the detection performance, we model the ICPs using Markov random field. We experimentally evaluate the performance of our method and demonstrate that it works in real-time.},
	booktitle = {Proc. 19th {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Koyama, Tatsuya and Nakashima, Yuta and Babaguchi, Noboru},
	month = sep,
	year = {2012},
	keywords = {camera motion estimation, cameras, Cameras, Capture intention, Color, Feature extraction, inertial sensor, inertial sensors, intentionally-captured person, intentionally-captured persons, Iterative closest point algorithm, Markov processes, Markov random field, Markov random field-based real-time detection, motion estimation, object detection, pairwise constraints, person motion, person size, Sensors, Skin, video signal processing, videographer, Videos},
	pages = {1377--1380},
	file = {IEEE Xplore Abstract Record:files/47/6467125.html:text/html}
}

@article{___2013,
	title = {拡張現実感のための視点依存テクスチャ・ジオメトリに基づく仮想化実物体の輪郭形状の修復 (パターン認識・メディア理解)},
	volume = {112},
	issn = {0913-5685},
	url = {https://ci.nii.ac.jp/naid/110009728299},
	number = {385},
	urldate = {2019-04-30},
	journal = {電子情報通信学会 技術研究報告, MVE2012-74},
	author = {{宇野 祐介} and {中島 悠太} and {河合 紀彦} and {佐藤 智和} and {横矢 直和}},
	month = jan,
	year = {2013},
	keywords = {仮想化実物体, 拡張現実感, 視点依存ジオメトリ, 視点依存テクスチャ, 自由視点画像},
	pages = {223--228},
	file = {拡張現実感のための視点依存テクスチャ・ジオメトリに基づく仮想化実物体の輪郭形状の修復 (パターン認識・メディア理解) Snapshot:files/49/110009728299.html:text/html}
}

@inproceedings{uno_generating_2013,
	title = {Generating augmented reality images with a real object using view-dependent texture and geometry},
	booktitle = {Proc. 6th {Korea}-{Japan} {Workshop} on {Mixed} {Reality} ({KJMR})},
	author = {Uno, Yusuke and Nakashima, Yuta and Kawai, Norihiko and Sato, Tomokazu and Yokoya, Naokazu},
	month = apr,
	year = {2013},
	pages = {6}
}

@inproceedings{koyama_real-time_2013,
	title = {Real-time privacy protection system for social videos using intentionally-captured persons detection},
	doi = {10.1109/ICME.2013.6607622},
	abstract = {Most social videos, which are uploaded and shared through social networking services (SNSs), e.g., YouTube and Facebook, contain not only intentionally-captured persons (ICPs) but also non-ICPs who are unexpectedly framed in, such as passers-by. Sharing such social videos may infringe on the non-ICPs' privacy but not on the ICPs' in many cases; however, existing systems for video privacy protection simply obscure persons without distinguishing ICPs from non-ICPs. This naive obscuration may spoil the videos. Since this is a critical problem especially for social videos, in this paper, we propose a novel system for automatically generating privacy-protected videos in real-time. Our system localizes ICPs and non-ICPs using ICP detection leveraging the spatial and temporal consistency of ICPs/non-ICPs and obscures the non-ICPs. We have experimentally evaluated the performance of ICP detection and demonstrated the applicability of our system.},
	booktitle = {Proc. 2013 {IEEE} {International} {Conference} on {Multimedia} and {Expo} ({ICME})},
	author = {Koyama, Tatsuya and Nakashima, Yuta and Babaguchi, Noboru},
	month = jul,
	year = {2013},
	keywords = {Abstracts, data privacy, ICP, Intentionally-captured person, intentionally-captured persons, intentionally-captured persons detection, nonICP, nonintentionally-captured persons, privacy protection, privacy-protected videos, Random access memory, real-time, real-time privacy protection system, security of data, social networking (online), social networking services, social videos, video signal processing},
	pages = {6},
	file = {IEEE Xplore Abstract Record:files/52/6607622.html:text/html}
}

@inproceedings{nakashima_inferring_2013,
	title = {Inferring what the videographer wanted to capture},
	doi = {10.1109/ICIP.2013.6738040},
	abstract = {Detecting important regions in videos has been extensively studied for past decades for their wide variety of applications including video summarization and retargeting. Visual attention models draw much attention for this purpose, which find visually salient regions. However, visual attention models ignore intentionally captured regions (ICRs) derived from videographers' intentions, i.e., what the videographers wanted to capture in their videos. This paper proposes a Markov random field-based ICR model for finding them. Observing that a videographer's intention is embedded into camera motion together with objects' motion, our ICR model uses point trajectory-based features to distinguish ICRs from non-ICRs. It also leverages spatial and temporal consistency of ICRs to improve the performance. We have experimentally demonstrated our ICR model's performance and the difference between ICRs and visually salient regions.},
	booktitle = {Proc. 2013 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Nakashima, Yuta and Yokoya, Naokazu},
	month = sep,
	year = {2013},
	keywords = {camera motion, cameras, Cameras, capture intentions, Computer vision, image motion analysis, intention map, intentionally captured regions, Intentionally captured regions, Markov processes, Markov random field-based ICR model, object detection, object motion, point trajectory-based features, random processes, region detection, Support vector machines, Trajectory, Vectors, video retargeting, video signal processing, video summarization, videographer intention, Videos, visual attention model, visual attention models, Visualization, visually salient regions},
	pages = {191--195},
	file = {IEEE Xplore Abstract Record:files/54/6738040.html:text/html;Nakashima and Yokoya - 2013 - Inferring what the videographer wanted to capture.pdf:files/55/Nakashima and Yokoya - 2013 - Inferring what the videographer wanted to capture.pdf:application/pdf}
}

@inproceedings{nakashima_augmented_2013,
	title = {Augmented reality image generation with virtualized real objects using view-dependent texture and geometry},
	doi = {10.1109/ISMAR.2013.6671827},
	abstract = {Augmented reality (AR) images with virtualized real objects can be used for various applications. However, such AR image generation requires hand-crafted 3D models of that objects, which are usually not available. This paper proposes a view-dependent texture (VDT)- and view-dependent geometry (VDG)-based method for generating high quality AR images, which uses 3D models automatically reconstructed from multiple images. Since the quality of reconstructed 3D models is usually insufficient, the proposed method inflates the objects in the depth map as VDG to repair chipped object boundaries and assigns a color to each pixel based on VDT to reproduce the detail of the objects. Background pixel exposure due to inflation is suppressed by the use of the foreground region extracted from the input images. Our experimental results have demonstrated that the proposed method can successfully reduce above visual artifacts.},
	booktitle = {Proc. 2013 {IEEE} {International} {Symposium} on {Mixed} and {Augmented} {Reality} ({ISMAR})},
	author = {Nakashima, Yuta and Sato, Tomokazu and Uno, Yusuke and Yokoya, Naokazu and Kawai, Norihiko},
	month = oct,
	year = {2013},
	keywords = {AR with virtualized real objects, view-dependent geometry, view-dependent texture},
	pages = {6},
	file = {IEEE Xplore Abstract Record:files/57/6671827.html:text/html}
}

@techreport{___2014,
	title = {自由視点画像生成に基づく移動撮影した全方位動画像からの動物体除去},
	url = {https://ci.nii.ac.jp/naid/110009830023},
	number = {電子情報通信学会2014年総合大会講演論文集, D-11-43},
	urldate = {2019-04-30},
	author = {{井上 直哉} and {河合 紀彦} and {佐藤 智和} and {大倉 史生} and {中島 悠太} and 横矢 直和},
	month = mar,
	year = {2014},
	pages = {1},
	file = {D-11-43 自由視点画像生成に基づく移動撮影した全方位動画像からの動物体除去(D-11.画像工学,一般セッション) Snapshot:files/59/110009830023.html:text/html}
}

@techreport{___2014-1,
	title = {画像のコンテキストを保持した視覚的に自然なプライバシー保護処理 (パターン認識・メディア理解)},
	url = {https://ci.nii.ac.jp/naid/110009862548},
	abstract = {スマートフォンなどで撮影された画像には，撮影者が意図的にフレームインさせた人物（意図人物）とそれ以外の人物（非意図人物）が存在する．非意図人物から画像を公開する許可を得るのは困難であるため，画像をインターネット上に配信する際には非意図人物に対してプライバシー保護処理を適用する必要がある．しかし，プライバシー保護処理として広く用いられるぼかしや塗りつぶしでは，保護領域が視覚的に不自然であり，さらに意図人物の状況を理解するために必要な情報（画像のコンテキスト）のひとつである非意図人物の表情が読み取れなくなる．そこで本稿では，非意図人物の表情を保持することにより，画像のコンテキストを保持した視覚的に自然なプライバシー保護処理手法を提案する．また，提案手法の性能を実験により明らかにする．},
	urldate = {2019-04-30},
	author = {{小山 達也} and {中島 悠太} and {馬場口 登}},
	month = mar,
	year = {2014},
	keywords = {Image Melding, プライバシー保護, 撮影意図, 画像のコンテキスト, 表情},
	pages = {217--222},
	file = {画像のコンテキストを保持した視覚的に自然なプライバシー保護処理 (パターン認識・メディア理解) Snapshot:files/61/110009862548.html:text/html}
}

@inproceedings{dayrit_free-viewpoint_2014,
	title = {Free-viewpoint {AR} human-motion reenactment based on a single {RGB}-{D} video stream},
	doi = {10.1109/ICME.2014.6890243},
	abstract = {When observing a person (an actor) performing or demonstrating some activity for the purpose of learning the action, it is best for the viewers to be present at the same time and place as the actor. Otherwise, a video must be recorded. However, conventional video only provides two-dimensional (2D) motion, which lacks the original third dimension of motion. In the presence of some ambiguity, it may be hard for the viewer to comprehend the action with only two dimensions, making it harder to learn the action. This paper proposes an augmented reality system to reenact such actions at any time the viewer wants, in order to aid comprehension of 3D motion. In the proposed system, a user first captures the actor's motion and appearance, using a single RGB-D camera. Upon a viewer's request, our system displays the motion from an arbitrary viewpoint using a rough 3D model of the subject, made up of cylinders, and selecting the most appropriate textures based on the viewpoint and the subject's pose. We evaluate the usefulness of the system and the quality of the displayed images by user study.},
	booktitle = {Proc. 2014 {IEEE} {International} {Conference} on {Multimedia} and {Expo} ({ICME})},
	author = {Dayrit, Fabian Lorenzo and Nakashima, Yuta and Sato, Tomokazu and Yokoya, Naokazu},
	month = jul,
	year = {2014},
	keywords = {3D motion comprehension, action learning, augmented reality, Augmented reality, augmented reality system, Cameras, free-viewpoint AR human-motion reenactment, free-viewpoint image generation, human motion capture, image colour analysis, image motion analysis, image sensors, image texture, Joints, rough 3D model, Sensors, single RGB-D camera, single RGB-D video stream, Solid modeling, solid modelling, Streaming media, texture selection, Three-dimensional displays, video signal processing},
	pages = {1--6},
	file = {Dayrit et al. - 2014 - Free-viewpoint AR human-motion reenactment based o.pdf:files/64/Dayrit et al. - 2014 - Free-viewpoint AR human-motion reenactment based o.pdf:application/pdf;IEEE Xplore Abstract Record:files/63/6890243.html:text/html}
}

@article{kawai_background_2014,
	title = {Background estimation for a single omnidirectional image sequence captured with a moving camera},
	volume = {6},
	issn = {1882-6695},
	url = {https://www.jstage.jst.go.jp/article/ipsjtcva/6/0/6_68/_article},
	doi = {10.2197/ipsjtcva.6.68},
	abstract = {This paper proposes a background estimation method from a single omnidirectional image sequence for removing undesired regions such as moving objects, specular regions, and uncaptured regions caused by the camera's blind spot without manual specification. The proposed method aligns multiple frames using a reconstructed 3D model of the environment and generates background images by minimizing an energy function for selecting a frame for each pixel. In the energy function, we introduce patch similarity and camera positions to remove undesired regions more correctly and generate high-resolution images. In experiments, we demonstrate the effectiveness of the proposed method by comparing the result given by the proposed method with those from conventional approaches.},
	language = {en},
	urldate = {2019-04-30},
	journal = {IPSJ Transactions on Computer Vision and Applications},
	author = {Kawai, Norihiko and Inoue, Naoya and Sato, Tomokazu and Okura, Fumio and Nakashima, Yuta and Yokoya, Naokazu},
	month = jul,
	year = {2014},
	pages = {68--72},
	file = {Full Text PDF:files/66/Kawai et al. - 2014 - Background Estimation for a Single Omnidirectional.pdf:application/pdf;Snapshot:files/67/_article.html:text/html}
}

@techreport{___2014-2,
	title = {特徴点の類似度尺度による対応付けを伴わないカメラ位置姿勢推定手法の検討},
	url = {https://ci.nii.ac.jp/naid/110009933786},
	number = {映像情報メディア学会2014年年次大会講演予稿集, 3-3},
	urldate = {2019-04-30},
	author = {{黒川 陽平} and {中島 悠太} and {佐藤 智和} and {横矢 直和}},
	month = sep,
	year = {2014},
	pages = {3--3--1--3--3--2},
	file = {3-3 特徴点の類似度尺度による対応付けを伴わないカメラ位置姿勢推定手法の検討(第3部門 コンピュータビジョン) Snapshot:files/70/110009933786.html:text/html}
}

@techreport{___2014-3,
	title = {テキスト記述を用いてユーザ意図を反映する映像要約},
	number = {電気関係学会2014年関西連合大会講演論文集},
	author = {{大谷 まゆ} and {中島 悠太} and {佐藤 智和} and {横矢 直和}},
	month = nov,
	year = {2014},
	pages = {384--385}
}

@techreport{__rgb-d3_2014,
	title = {{RGB}-{Dカメラを用いた非剛体物体の動き復元のための}3次元テンプレート形状生成},
	url = {https://ci.nii.ac.jp/naid/110009933744},
	number = {映像情報メディア学会2014年冬季大会講演予稿集},
	urldate = {2019-04-30},
	author = {{武原 光} and {中島 悠太} and {佐藤 智和} and {河合 紀彦} and {横矢 直和}},
	year = {2014},
	pages = {10--2--1--10--2--2},
	file = {10-2 RGB-Dカメラを用いた非剛体物体の動き復元のための3次元テンプレート形状生成(第10部門 メディア工学(AR・画像認識)) Snapshot:files/73/110009933744.html:text/html}
}

@article{babaguchi_protection_2015,
	title = {Protection and utilization of privacy information via sensing},
	volume = {E98.D},
	issn = {0916-8532, 1745-1361},
	url = {https://www.jstage.jst.go.jp/article/transinf/E98.D/1/E98.D_2014MUI0001/_article},
	doi = {10.1587/transinf.2014MUI0001},
	abstract = {Our society has been getting more privacy-sensitive. Diverse information is given by users to information and communications technology (ICT) systems such as IC cards benefiting them. The information is stored as so-called big data, and there is concern over privacy violation. Visual information such as images and videos is also considered privacy-sensitive. The growing deployment of surveillance cameras and social network services has caused a privacy problem of information given from various sensors. To protect privacy of subjects presented in visual information, their face or figure is processed by means of pixelization or blurring. As image analysis technologies have made considerable progress, many attempts to automatically process flexible privacy protection have been made since 2000, and utilization of privacy information under some restrictions has been taken into account in recent years. This paper addresses the recent progress of privacy protection for visual information, showing our research projects: PriSurv, Digital Diorama (DD), and Mobile Privacy Protection (MPP). Furthermore, we discuss Harmonized Information Field (HIFI) for appropriate utilization of protected privacy information in a specific area.},
	language = {en},
	number = {1},
	urldate = {2019-04-30},
	journal = {IEICE Transactions on Information and Systems},
	author = {Babaguchi, Noboru and Nakashima, Yuta},
	month = jan,
	year = {2015},
	pages = {2--9},
	file = {Full Text PDF:files/75/Babaguchi and Nakashima - 2015 - Protection and Utilization of Privacy Information .pdf:application/pdf;Snapshot:files/76/_article.html:text/html}
}

@techreport{___2015,
	title = {テキストと映像の類似度を用いた映像要約},
	url = {https://ci.nii.ac.jp/naid/170000092555},
	abstract = {安価なカメラの普及により映像撮影および映像をインターネット上で公開することが一般的となった．近年，そのような映像の利用方法として，ビデオブログが注目されている．しかし，ピデオブログ制作において，大量の映像からユーザの意図に沿った映像に編集する作業には膨大な手間がかかる．本研究では，このような映像編集にかかる労力を削減するため，新たな映像要約手法を提案する．提案手法は，ユーザがブログ記事のために執筆したテキストに類似した内容の要約映像を生成することで，そのブログ記事に合った映像を生成する．そのために，映像要約を映像とテキストの類似度に関する最適化問題として定式化する．実験では，20 人の被験者に対するユーザスタディにより，ビデオブログのための映像作成支援として提案手法が有効であることを確認する．},
	number = {電子情報通信学会 技術研究報告, PRMU2014-95},
	urldate = {2019-04-30},
	author = {{大谷 まゆ} and {中島 悠太} and {佐藤 智和} and {横矢 直和}},
	month = jan,
	year = {2015},
	pages = {６},
	file = {テキストと映像の類似度を用いた映像要約 Snapshot:files/78/170000092555.html:text/html}
}

@techreport{___2015-1,
	title = {特徴点の明示的な対応付けを伴わないカメラ位置姿勢推定},
	url = {https://ci.nii.ac.jp/naid/110009882580},
	abstract = {モバイル端末での利用を想定した軽量なカメラ位置姿勢推定の実現に向けて，本稿では類似度による特徴点の明示的な対応付けを伴わない手法について検討する．提案手法では，事前に生成した 3 次元点群データベースに含まれる特徴点を対象画像上に投影して得られる投影点と対象画像から得られる特徴点の画像上での位置の一致度合いを評価尺度とし，これに基づいてカメラの位置姿勢を推定する．},
	number = {情報処理学会 研究報告, CVIM-195-60},
	urldate = {2019-04-30},
	author = {{黒川 陽平} and {中島 悠太} and {佐藤 智和} and {横矢 直和}},
	month = jan,
	year = {2015},
	keywords = {ポスターセッション},
	pages = {4},
	file = {特徴点の明示的な対応付けを伴わないカメラ位置姿勢推定 Snapshot:files/80/110009882580.html:text/html}
}

@techreport{__rgb-drgb3_2015,
	title = {{RGB}-{Dカメラを用いた非剛体物体の動き復元のためのRGB画像上の対応点に基づく}3次元テンプレート生成},
	url = {https://ci.nii.ac.jp/naid/110009882565},
	abstract = {近年，非剛体物体を撮影した RGB-D 画像から 3 次元テンプレートに基づいてその物体の動きを復元する手法が提案されている．しかしこれらの手法では，はじめに対象物が変形しない状態で 3 次元テンプレートを生成する必要がある．本研究では，非剛体物体の動きの復元を目指し，運動中の物体を撮影した RGB-D 画像群から 3 次元テンプレートを生成する手法を提案する．フレーム間での非剛体運動を考慮した物体の位置合わせは，推定すべき運動パラメータの初期値に大きく依存し，位置合わせの安定性が低いという問題がある．提案手法では，RGB 画像上の対応点を援用することにより安定した位置合わせを実現する．},
	number = {情報処理学会 研究報告, CVIM-195-45},
	urldate = {2019-04-30},
	author = {{武原 光} and {中島 悠太} and {佐藤 智和} and {河合 紀彦} and {横矢 直和}},
	month = jan,
	year = {2015},
	pages = {8},
	file = {RGB-Dカメラを用いた非剛体物体の動き復元のためのRGB画像上の対応点に基づく3次元テンプレート生成 Snapshot:files/82/110009882565.html:text/html}
}

@techreport{___2015-2,
	title = {テクスチャの連続性を考慮した視点依存テクスチャマッピングによる自由視点画像生成 (パターン認識・メディア理解)},
	url = {https://ci.nii.ac.jp/naid/110010022170},
	abstract = {自由視点画像生成とは，あるシーンを複数の地点から撮影した画像を入力として，任意の視点からの見えを再現した画像を生成する技術である．この技術は，遠隔地のシーンを仮想的に体感できるテレプレゼンス等のアプリケーションでの活用が期待されている．本研究では，視点依存テクスチャマッピングにおけるテクスチャ選択処理において，出力画像上でのテクスチャの連続性を考慮することで，従来手法で生じていたテクスチャや構造のぼけを抑止する手法を提案する．},
	number = {電子情報通信学会 技術研究報告, PRMU2014-162},
	urldate = {2019-04-30},
	author = {{片桐 敬太} and {中島 悠太} and {佐藤 智和} and {横矢 直和}},
	month = mar,
	year = {2015},
	keywords = {グラフカット, テクスチャの連続性, 視点依存テクスチャマッピング, 自由視点画像生成},
	pages = {17--22},
	file = {テクスチャの連続性を考慮した視点依存テクスチャマッピングによる自由視点画像生成 (パターン認識・メディア理解) Snapshot:files/84/110010022170.html:text/html}
}

@article{nakashima_ar_2015,
	title = {{AR} image generation using view-dependent geometry modification and texture mapping},
	volume = {19},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-015-0259-3},
	doi = {10.1007/s10055-015-0259-3},
	abstract = {Augmented reality (AR) applications often require virtualized real objects, i.e., virtual objects that are built based on real objects and rendered from an arbitrary viewpoint. In this paper, we propose a method for real object virtualization and AR image generation based on view-dependent geometry modification and texture mapping. The proposed method is a hybrid of model- and image-based rendering techniques that uses multiple input images of the real object as well as the object’s three-dimensional (3D) model obtained by an automatic 3D reconstruction technique. Even with state-of-the-art technology, the reconstructed 3D model’s accuracy can be insufficient, resulting in such visual artifacts as false object boundaries. The proposed method generates a depth map from a 3D model of a virtualized real object and expands its region in the depth map to remove the false object boundaries. Since such expansion reveals the background pixels in the input images, which is particularly undesirable for AR applications, we preliminarily extract object regions and use them for texture mapping. With our GPU implementation for real-time AR image generation, we experimentally demonstrated that using expanded geometry reduces the number of required input images and maintains visual quality.},
	language = {en},
	number = {2},
	urldate = {2019-04-30},
	journal = {Virtual Reality},
	author = {Nakashima, Yuta and Uno, Yusuke and Kawai, Norihiko and Sato, Tomokazu and Yokoya, Naokazu},
	month = jun,
	year = {2015},
	keywords = {Augmented reality, Free-viewpoint image generation, View-dependent geometry modification, View-dependent texture mapping},
	pages = {83--94},
	file = {Nakashima et al. - 2015 - AR image generation using view-dependent geometry .pdf:files/88/Nakashima et al. - 2015 - AR image generation using view-dependent geometry .pdf:application/pdf}
}

@inproceedings{nakashima_facial_2015,
	title = {Facial expression preserving privacy protection using image melding},
	doi = {10.1109/ICME.2015.7177394},
	abstract = {An enormous number of images are currently shared through social networking services such as Facebook. These images usually contain appearance of people and may violate the people's privacy if they are published without permission from each person. To remedy this privacy concern, visual privacy protection, such as blurring, is applied to facial regions of people without permission. However, in addition to image quality degradation, this may spoil the context of the image: If some people are filtered while the others are not, missing facial expression makes comprehension of the image difficult. This paper proposes an image melding-based method that modifies facial regions in a visually unintrusive way with preserving facial expression. Our experimental results demonstrated that the proposed method can retain facial expression while protecting privacy.},
	booktitle = {Proc. 2015 {IEEE} {International} {Conference} on {Multimedia} and {Expo} ({ICME})},
	author = {Nakashima, Yuta and Koyama, Tatsuya and Yokoya, Naokazu and Babaguchi, Noboru},
	month = jun,
	year = {2015},
	keywords = {Avatars, data protection, face recognition, Face recognition, Facebook, facial expression, facial expression preserving privacy protection, Facial features, facial regions, image context, image melding-based method, image quality degradation, Image recognition, Privacy, social networking services, Target recognition, visual privacy protection, Visual privacy protection, Visualization},
	pages = {6},
	file = {IEEE Xplore Abstract Record:files/90/7177394.html:text/html;Nakashima et al. - 2015 - Facial expression preserving privacy protection us.pdf:files/91/Nakashima et al. - 2015 - Facial expression preserving privacy protection us.pdf:application/pdf}
}

@inproceedings{otani_textual_2015,
	title = {Textual description-based video summarization for video blogs},
	url = {https://ieeexplore.ieee.org/document/7177493},
	doi = {10.1109/ICME.2015.7177493},
	abstract = {Recent popularization of camera devices, including action cams and smartphones, enables us to record videos in everyday life and share them through the Internet. Video blog is a recent approach for sharing videos, in which users enjoy expressing themselves in blog posts with attractive videos. Generating such videos, however, requires users to review vast amount of raw videos and edit them appropriately, which keeps users away from doing so. In this paper, we propose a novel video summarization method for helping users to create a video blog post. Unlike typical video summarization methods, the proposed method utilizes the text, which is written for a video blog post, and makes the video summary consistent with the content of the text. For this, we perform video summarization by solving an optimization problem, in which an objective function involves the content similarity between the summarized video and the text. Our user study with 20 participants has demonstrated that our proposed method is suitable to create video blog posts compared with conventional methods for video summarization.},
	booktitle = {Proc. 2015 {IEEE} {International} {Conference} on {Multimedia} and {Expo}},
	author = {Otani, Mayu and Nakashima, Yuta and Sato, Tomokazu and Yokoya, Naokazu},
	month = jun,
	year = {2015},
	keywords = {action cameras, blog posts, Blogs, camera devices, Cameras, Internet, Linear programming, objective function, optimisation, optimization problem, Redundancy, Rivers, smart phones, text analysis, text content, textual description-based video summarization, user study, Video blog, video blog post, video generation, video sharing, video signal processing, video summarization, video summarization methods, Visualization, Web sites},
	pages = {6},
	file = {IEEE Xplore Abstract Record:files/93/7177493.html:text/html;Otani et al. - 2015 - Textual description-based video summarization for .pdf:files/94/Otani et al. - 2015 - Textual description-based video summarization for .pdf:application/pdf}
}

@techreport{nakashima_point_2015,
	title = {Point trajectory-based inference of what the videographer wanted to capture},
	number = {画像の認識・理解シンポジウム（MIRU）講演論文集, SS1-13},
	author = {Nakashima, Yuta and Yokoya, Naokazu},
	month = jul,
	year = {2015},
	pages = {2}
}

@techreport{keita_katagiri_view-dependent_2015,
	title = {View-dependent texture mapping-based novel view synthesis with geometry-aware color continuity},
	number = {画像の認識・理解シンポジウム（MIRU）講演論文集, SS1-12},
	author = {{Keita Katagiri} and {Yuta Nakashima} and {Tomokazu Sato} and {Naokazu Yokoya}},
	month = jul,
	year = {2015},
	pages = {2}
}

@techreport{__rgb-d3_2015,
	title = {{単一のRGB}-{Dカメラを用いた非剛体物体の}3次元形状復元},
	url = {https://ci.nii.ac.jp/naid/40020609071},
	number = {第32回センシングフォーラム},
	urldate = {2019-04-30},
	author = {{武原 光} and {中島 悠太} and {佐藤 智和} and {横矢 直和}},
	month = sep,
	year = {2015},
	pages = {135--140},
	file = {単一のRGB-Dカメラを用いた非剛体物体の3次元形状復元 Snapshot:files/98/40020609071.html:text/html}
}

@techreport{___2015-3,
	title = {ユーザ意図反映のためのテキストに基づく映像要約手法},
	number = {電気関係学会2015年関西連合大会講演論文集, P-25},
	author = {{大谷 まゆ} and {中島 悠太} and {佐藤 智和} and {横矢 直和}},
	month = nov,
	year = {2015},
	pages = {388--389}
}

@techreport{___2015-4,
	title = {画像修復における畳み込みニューラルネットワークを用いた修復失敗領域の自動検出},
	number = {映像情報メディア学会2015年冬季大会講演予稿集, 11B-3},
	urldate = {2019-04-30},
	author = {{田中 隆寛} and {河合 紀彦} and {中島 悠太} and {佐藤 智和} and {横矢 直和}},
	month = dec,
	year = {2015},
	keywords = {automatic detection of failure regions, convolutional neural network, image inpainting, 修復失敗領域の自動検出, 画像修復, 畳み込みニューラルネットワーク},
	pages = {2},
	file = {田中 隆寛 et al. - 2016 - 畳み込みニューラルネットワークを用いた修復失敗領域の自動検出による画像修復の反復的適用 (パターン認.pdf:files/102/田中 隆寛 et al. - 2016 - 畳み込みニューラルネットワークを用いた修復失敗領域の自動検出による画像修復の反復的適用 (パターン認.pdf:application/pdf;畳み込みニューラルネットワークを用いた修復失敗領域の自動検出による画像修復の反復的適用 (パターン認識・メディア理解) Snapshot:files/101/40020758619.html:text/html}
}

@article{nakashima_evaluating_2016,
	title = {Evaluating protection capability for visual privacy information},
	volume = {14},
	issn = {1540-7993},
	url = {https://ieeexplore.ieee.org/document/7397722},
	doi = {10.1109/MSP.2016.3},
	abstract = {One way to prevent privacy intrusion is by blurring or blocking out facial images using image processing. However, this technique's effectiveness depends on viewers' familiarity with the subjects as well as on the subjects' conspicuousness.},
	number = {1},
	journal = {IEEE Security Privacy},
	author = {Nakashima, Yuta and Ikeno, Tomoaki and Babaguchi, Noboru},
	month = feb,
	year = {2016},
	keywords = {Cameras, Computer vision, data protection, evaluation, face recognition, Face recognition, facial images, image processing, Image processing, person conspicuousness, privacy, Privacy, privacy intrusion prevention, privacy protection, protection capability evaluation, Surveillance, viewer familiarity, visual privacy information, Visualization},
	pages = {55--61},
	file = {IEEE Xplore Abstract Record:files/104/7397722.html:text/html;Nakashima et al. - 2016 - Evaluating protection capability for visual privac.pdf:files/105/Nakashima et al. - 2016 - Evaluating protection capability for visual privac.pdf:application/pdf}
}

@inproceedings{takehara_3d_2016,
	title = {3D shape template generation from {RGB}-{D} images capturing a moving and deforming object},
	volume = {2016},
	url = {http://www.ingentaconnect.com/content/10.2352/ISSN.2470-1173.2016.21.3DIPM-407},
	doi = {10.2352/ISSN.2470-1173.2016.21.3DIPM-407},
	abstract = {Automatically reconstructing a 3D shape model of a nonrigid object using a sequence from a single commodity RGB-D sensor is a challenging problem. Some techniques use a 3D shape template of a target object; however, in order to generate the template automatically, the target object required to be stationary. Otherwise, a non-rigid ICP algorithm, which registers a pair of point clouds, can be used for reconstructing 3D geometry of a non-rigid object directly, but it often fails due to the ambiguity in point correspondences. This paper presents a method for generating a 3D shape template from a single RGB-D sequence. In order to reduce the ambiguity in point correspondences, our method leverages point trajectories obtained in the RGB images, which can be used for associating points in different point clouds. We demonstrate the capability of our method using deforming human bodies.},
	language = {en},
	urldate = {2019-04-30},
	booktitle = {Electronic {Imaging}},
	author = {Takehara, Hikari and Nakashima, Yuta and Sato, Tomokazu and Yokoya, Naokazu},
	month = feb,
	year = {2016},
	pages = {7},
	file = {Takehara et al. - 2016 - 3D shape template generation from RGB-D images cap.pdf:files/108/Takehara et al. - 2016 - 3D shape template generation from RGB-D images cap.pdf:application/pdf}
}

@techreport{___2016,
	title = {畳み込みニューラルネットワークを用いた修復失敗領域の自動検出による画像修復の反復的適用 (クラウドネットワークロボット)},
	url = {https://ci.nii.ac.jp/naid/40020758848},
	number = {電子情報通信学会 技術研究報告, PRMU2015-160},
	urldate = {2019-04-30},
	author = {{田中 隆寛} and {河合 紀彦} and {中島 悠太} and {佐藤 智和} and {横矢 直和}},
	month = feb,
	year = {2016},
	keywords = {automatic detection of failure regions, convolutional neural network, image inpainting, 修復失敗領域の自動検出, 画像修復, 畳み込みニューラルネットワーク},
	pages = {133--138},
	file = {畳み込みニューラルネットワークを用いた修復失敗領域の自動検出による画像修復の反復的適用 (クラウドネットワークロボット) Snapshot:files/111/40020758848.html:text/html}
}

@article{katagiri_novel_2016,
	title = {Novel {View} {Synthesis} {Based} on {View}-dependent {Texture} {Mapping} with {Geometry}-aware {Color} {Continuity}({\textless}{Special} issue{\textgreater}{VR} {Psychology} {VI})},
	volume = {21},
	issn = {1344-011X, 2423-9593},
	url = {https://www.jstage.jst.go.jp/article/tvrsj/21/1/21_KJ00010255263/_article/-char/ja/},
	doi = {10.18974/tvrsj.21.1_153},
	abstract = {J-STAGE},
	language = {en},
	number = {1},
	urldate = {2019-04-30},
	journal = {Transactions of the Virtual Reality Society of Japan},
	author = {Katagiri, Keita and Nakashima, Yuta and Sato, Tomokazu and Yokoya, Naokazu},
	month = mar,
	year = {2016},
	pages = {153--162},
	file = {Full Text PDF:files/113/Katagiri et al. - 2016 - Novel View Synthesis Based on View-dependent Textu.pdf:application/pdf;Snapshot:files/114/ja.html:text/html}
}

@article{nakashima_privacy_2016,
	title = {Privacy protection for social video via background estimation and {CRF}-based videographer's intention modeling},
	volume = {E99.D},
	issn = {0916-8532, 1745-1361},
	url = {https://www.jstage.jst.go.jp/article/transinf/E99.D/4/E99.D_2015EDP7378/_article},
	doi = {10.1587/transinf.2015EDP7378},
	abstract = {The recent popularization of social network services (SNSs), such as YouTube, Dailymotion, and Facebook, enables people to easily publish their personal videos taken with mobile cameras. However, at the same time, such popularity has raised a new problem: video privacy. In such social videos, the privacy of people, i.e., their appearances, must be protected, but naively obscuring all people might spoil the video content. To address this problem, we focus on videographers' capture intentions. In a social video, some persons are usually essential for the video content. They are intentionally captured by the videographers, called intentionally captured persons (ICPs), and the others are accidentally framed-in (non-ICPs). Videos containing the appearances of the non-ICPs might violate their privacy. In this paper, we developed a system called BEPS, which adopts a novel conditional random field (CRF)-based method for ICP detection, as well as a novel approach to obscure non-ICPs and preserve ICPs using background estimation. BEPS reduces the burden of manually obscuring the appearances of the non-ICPs before uploading the video to SNSs. Compared with conventional systems, the following are the main advantages of BEPS: (i) it maintains the video content, and (ii) it is immune to the failure of person detection; false positives in person detection do not violate privacy. Our experimental results successfully validated these two advantages.},
	language = {en},
	number = {4},
	urldate = {2019-04-30},
	journal = {IEICE Transactions on Information and Systems},
	author = {Nakashima, Yuta and Babaguchi, Noboru and Fan, Jianping},
	month = apr,
	year = {2016},
	pages = {1221--1233},
	file = {Full Text PDF:files/116/Nakashima et al. - 2016 - Privacy Protection for Social Video via Background.pdf:application/pdf;Snapshot:files/117/_article.html:text/html}
}

@article{tejero-de-pablos_flexible_2016,
	title = {Flexible human action recognition in depth video sequences using masked joint trajectories},
	volume = {2016},
	issn = {1687-5281},
	url = {https://doi.org/10.1186/s13640-016-0120-y},
	doi = {10.1186/s13640-016-0120-y},
	abstract = {Human action recognition applications are greatly benefited from the use of commodity depth sensors that are capable of skeleton tracking. Some of these applications (e.g., customizable gesture interfaces) require learning of new actions at runtime and may not count with many training instances. This paper presents a human action recognition method designed for flexibility, which allows taking users’ feedback to improve recognition performance and to add a new action instance without computationally expensive optimization for training classifiers. Our nearest neighbor-based action classifier adopts dynamic time warping to handle variability in execution rate. In addition, it uses the confidence values associated to each tracked joint position to mask erroneous trajectories for robustness against noise. We evaluate the proposed method with various datasets with different frame rates, actors, and noise. The experimental results demonstrate its adequacy for learning of actions from depth sequences at runtime. We achieve an accuracy comparable to the state-of-the-art techniques on the challenging MSR-Action3D dataset.},
	number = {1},
	urldate = {2019-04-30},
	journal = {EURASIP Journal on Image and Video Processing},
	author = {Tejero-de-Pablos, Antonio and Nakashima, Yuta and Yokoya, Naokazu and Díaz-Pernas, Francisco-Javier and Martínez-Zarzuela, Mario},
	month = jun,
	year = {2016},
	pages = {20},
	file = {Full Text PDF:files/119/Tejero-de-Pablos et al. - 2016 - Flexible human action recognition in depth video s.pdf:application/pdf;Snapshot:files/120/s13640-016-0120-y.html:text/html}
}

@inproceedings{tejero-de-pablos_human_2016,
	title = {Human action recognition-based video summarization for {RGB}-{D} personal sports video},
	url = {https://ieeexplore.ieee.org/document/7552938},
	doi = {10.1109/ICME.2016.7552938},
	abstract = {Automatic sports video summarization poses the challenge of acquiring semantics of the original video, and existing work leverages various knowledge in application domains, e.g., structure of games and editing conventions. In this paper, we propose a personal sports video summarization method for self-recorded RGB-D videos, which became available to the public due to the commodification of off-the-shelf RGB-D sensors. We focus on sports whose games consist of a succession of actions and, unlike previous research, we use human action recognition on the depth sequences in order to acquire higher level semantics of the video. The recognition results are used along with an entropy-based activity measure to train a hidden Markov model of the highlights of different games to extract a summary from the original RGB-D video. We trained our novel highlights model with the subjective opinion of users with different experience in the sport. We took Kendo, a martial art, as an example sport to evaluate our method, and objectively/subjectively investigated the accuracy and quality of the generated summaries.},
	booktitle = {Proc. 2016 {IEEE} {International} {Conference} on {Multimedia} and {Expo} ({ICME})},
	author = {Tejero-de-Pablos, Antonio and Nakashima, Yuta and Sato, Tomokazu and Yokoya, Naokazu},
	month = jul,
	year = {2016},
	keywords = {Art, automatic sport video summarization method, Cameras, entropy-based activity measurement, Games, hidden Markov model, hidden Markov models, Hidden Markov models, highlight extraction, human action recognition, human action recognition-based video summarization, Kendo, martial art, off-the-shelf RGB-D sensors, original video, personal sports video, pose estimation, RGB-D personal sport video, RGB-D video, Semantics, Sensors, Skeleton, sport, Video summarization},
	pages = {6},
	file = {IEEE Xplore Abstract Record:files/122/7552938.html:text/html}
}

@inproceedings{otani_learning_2016,
	title = {Learning joint representations of videos and sentences with web image search},
	url = {http://arxiv.org/abs/1608.02367},
	abstract = {Our objective is video retrieval based on natural language queries. In addition, we consider the analogous problem of retrieving sentences or generating descriptions given an input video. Recent work has addressed the problem by embedding visual and textual inputs into a common space where semantic similarities correlate to distances. We also adopt the embedding approach, and make the following contributions: First, we utilize web image search in sentence embedding process to disambiguate fine-grained visual concepts. Second, we propose embedding models for sentence, image, and video inputs whose parameters are learned simultaneously. Finally, we show how the proposed model can be applied to description generation. Overall, we observe a clear improvement over the state-of-the-art methods in the video and sentence retrieval tasks. In description generation, the performance level is comparable to the current state-of-the-art, although our embeddings were trained for the retrieval tasks.},
	urldate = {2019-04-30},
	booktitle = {Proc. 4th {Workshop} on {Web}-scale {Vision} and {Social} {Media} ({VSM}) at {ECCV}},
	author = {Otani, Mayu and Nakashima, Yuta and Rahtu, Esa and Heikkilä, Janne and Yokoya, Naokazu},
	month = aug,
	year = {2016},
	note = {arXiv: 1608.02367},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {16},
	annote = {Comment: 16 pages, 4th Workshop on Web-scale Vision and Social Media (VSM), ECCV 2016},
	file = {arXiv\:1608.02367 PDF:files/125/Otani et al. - 2016 - Learning Joint Representations of Videos and Sente.pdf:application/pdf;arXiv.org Snapshot:files/126/1608.html:text/html}
}

@inproceedings{otani_video_2016,
	title = {Video summarization using deep semantic features},
	url = {http://arxiv.org/abs/1609.08758},
	abstract = {This paper presents a video summarization technique for an Internet video to provide a quick way to overview its content. This is a challenging problem because finding important or informative parts of the original video requires to understand its content. Furthermore the content of Internet videos is very diverse, ranging from home videos to documentaries, which makes video summarization much more tough as prior knowledge is almost not available. To tackle this problem, we propose to use deep video features that can encode various levels of content semantics, including objects, actions, and scenes, improving the efficiency of standard video summarization techniques. For this, we design a deep neural network that maps videos as well as descriptions to a common semantic space and jointly trained it with associated pairs of videos and descriptions. To generate a video summary, we extract the deep features from each segment of the original video and apply a clustering-based summarization technique to them. We evaluate our video summaries using the SumMe dataset as well as baseline approaches. The results demonstrated the advantages of incorporating our deep semantic features in a video summarization technique.},
	urldate = {2019-04-30},
	booktitle = {Proc. 13th {Asian} {Conference} on {Computer} {Vision} ({ACCV})},
	author = {Otani, Mayu and Nakashima, Yuta and Rahtu, Esa and Heikkilä, Janne and Yokoya, Naokazu},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.08758},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {16},
	annote = {Comment: 16 pages, the 13th Asian Conference on Computer Vision (ACCV'16)},
	file = {arXiv\:1609.08758 PDF:files/129/Otani et al. - 2016 - Video Summarization using Deep Semantic Features.pdf:application/pdf;arXiv.org Snapshot:files/130/1609.html:text/html}
}

@inproceedings{dayrit_remagicmirror:_2017,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{ReMagicMirror}: {Action} learning using human reenactment with the mirror metaphor},
	isbn = {978-3-319-51811-4},
	shorttitle = {{ReMagicMirror}},
	abstract = {We propose ReMagicMirror, a system to help people learn actions (e.g., martial arts, dances). We first capture the motions of a teacher performing the action to learn, using two RGB-D cameras. Next, we fit a parametric human body model to the depth data and texture it using the color data, reconstructing the teacher’s motion and appearance. The learner is then shown the ReMagicMirror system, which acts as a mirror. We overlay the teacher’s reconstructed body on top of this mirror in an augmented reality fashion. The learner is able to intuitively manipulate the reconstruction’s viewpoint by simply rotating her body, allowing for easy comparisons between the learner and the teacher. We perform a user study to evaluate our system’s ease of use, effectiveness, quality, and appeal.},
	language = {en},
	booktitle = {Proc. {International} {Conference} on {MultiMedia} {Modeling} ({MMM})},
	publisher = {Springer International Publishing},
	author = {Dayrit, Fabian Lorenzo and Kimura, Ryosuke and Nakashima, Yuta and Blanco, Ambrosio and Kawasaki, Hiroshi and Ikeuchi, Katsushi and Sato, Tomokazu and Yokoya, Naokazu},
	editor = {Amsaleg, Laurent and Guðmundsson, Gylfi Þór and Gurrin, Cathal and Jónsson, Björn Þór and Satoh, Shin’ichi},
	month = jan,
	year = {2017},
	keywords = {3D human reconstruction, Human reenactment, RGB-D sensors},
	pages = {303--315}
}

@article{dayrit_increasing_2017,
	title = {Increasing pose comprehension through augmented reality reenactment},
	volume = {76},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-015-3116-1},
	doi = {10.1007/s11042-015-3116-1},
	abstract = {Standard video does not capture the 3D aspect of human motion, which is important for comprehension of motion that may be ambiguous. In this paper, we apply augmented reality (AR) techniques to give viewers insight into 3D motion by allowing them to manipulate the viewpoint of a motion sequence of a human actor using a handheld mobile device. The motion sequence is captured using a single RGB-D sensor, which is easier for a general user, but presents the unique challenge of synthesizing novel views using images captured from a single viewpoint. To address this challenge, our proposed system reconstructs a 3D model of the actor, then uses a combination of the actor’s pose and viewpoint similarity to find appropriate images to texture it. The system then renders the 3D model on the mobile device using visual SLAM to create a map in order to use it to estimate the mobile device’s camera pose relative to the original capturing environment. We call this novel view of a moving human actor a reenactment, and evaluate its usefulness and quality with an experiment and a survey.},
	language = {en},
	number = {1},
	urldate = {2019-04-30},
	journal = {Multimedia Tools and Applications},
	author = {Dayrit, Fabian Lorenzo and Nakashima, Yuta and Sato, Tomokazu and Yokoya, Naokazu},
	month = jan,
	year = {2017},
	keywords = {Augmented reality, Mobile, Novel view synthesis, Reenactment},
	pages = {1291--1312}
}

@techreport{__dnn6_2017,
	title = {{DNNを用いたカメラの}6自由度相対運動推定},
	url = {https://ipsj.ixsq.nii.ac.jp/ej/index.php?active_action=repository_view_main_item_detail&page_id=13&block_id=8&item_id=178340&item_no=1},
	abstract = {これまで複数枚の画像からカメラ運動とシーン構造を同時推定する Structure from Motion に関する手法が広く研究されてきた．多くの手法では画像上の特徴点を抽出し特徴の対応関係からカメラ運動とシーン構造を推定する．しかし，空や道路などのように画像上に特徴の少ないシーンでは，特徴点を検出できず推定に失敗することがある．このような理由から，より多様なシーンにおいてカメラの相対運動とシーンの 3 次元構造を頑健に推定可能な手法が求められている．本研究では，畳み込みニューラルネットワークを用い，2 枚の入力画像から直接カメラの相対運動を推定する手法を提案する．加えて，畳み込みニューラルネットワークで計算した特徴量を Long Short-Term Memory (LSTM) に入力することにより，時系列画像における過去の情報を考慮したカメラの相対運動を推定する手法を提案する．},
	language = {ja},
	number = {情報処理学会 研究報告, CVIM-206-13},
	urldate = {2019-04-30},
	author = {{橋岡 佳輝} and {大谷 まゆ} and {中島 悠太} and {佐藤 智和} and {横矢 直和}},
	month = mar,
	year = {2017},
	pages = {8},
	file = {Full Text PDF:files/134/佳輝 et al. - 2017 - DNNを用いたカメラの6自由度相対運動推定.pdf:application/pdf;Snapshot:files/135/ej.html:text/html}
}

@article{otani_video_2017,
	title = {Video summarization using textual descriptions for authoring video blogs},
	volume = {76},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-016-4061-3},
	doi = {10.1007/s11042-016-4061-3},
	abstract = {Authoring video blogs requires a video editing process, which is cumbersome for ordinary users. Video summarization can automate this process by extracting important segments from original videos. Because bloggers typically have certain stories for their blog posts, video summaries of a blog post should take the author’s intentions into account. However, most prior works address video summarization by mining patterns from the original videos without considering the blog author’s intentions. To generate a video summary that reflects the blog author’s intention, we focus on supporting texts in video blog posts and present a text-based method, in which the supporting text serves as a prior to the video summary. Given video and text that describe scenes of interest, our method segments videos and assigns to each video segment its priority in the summary based on its relevance to the input text. Our method then selects a subset of segments with content that is similar to the input text. Accordingly, our method produces different video summaries from the same set of videos, depending on the input text. We evaluated summaries generated from both blog viewers’ and authors’ perspectives in a user study. Experimental results demonstrate the advantages to the proposed text-based method for video blog authoring.},
	language = {en},
	number = {9},
	urldate = {2019-04-30},
	journal = {Multimedia Tools and Applications},
	author = {Otani, Mayu and Nakashima, Yuta and Sato, Tomokazu and Yokoya, Naokazu},
	month = may,
	year = {2017},
	keywords = {Text-based video summarization, User study, Video skimming},
	pages = {12097--12115},
	file = {Submitted Version:files/137/Otani et al. - 2017 - Video summarization using textual descriptions for.pdf:application/pdf}
}

@inproceedings{rongsirigul_novel_2017,
	address = {Hong Kong, Hong Kong},
	title = {Novel view synthesis with light-weight view-dependent texture mapping for a stereoscopic {HMD}},
	isbn = {978-1-5090-6067-2},
	url = {http://ieeexplore.ieee.org/document/8019417/},
	doi = {10.1109/ICME.2017.8019417},
	urldate = {2019-04-30},
	booktitle = {Proc. 2017 {IEEE} {International} {Conference} on {Multimedia} and {Expo} ({ICME})},
	publisher = {IEEE},
	author = {Rongsirigul, Thiwat and Nakashima, Yuta and Sato, Tomokazu and Yokoya, Naokazu},
	month = jul,
	year = {2017},
	pages = {703--708},
	file = {Submitted Version:files/139/Rongsirigul et al. - 2017 - Novel view synthesis with light-weight view-depend.pdf:application/pdf}
}

@inproceedings{otani_video_2017-1,
	title = {Video question answering to find a desired video segment},
	booktitle = {Proc. {Open} {Knowledge} {Base} and {Question} {Answering} {Workshop} ({OKBQA}) at {SIGIR}},
	author = {Otani, Mayu and Nakashima, Yuta and Rahtu, Esa and Heikkilä, Janne},
	month = aug,
	year = {2017},
	pages = {2}
}

@techreport{otani_unsupervised_2017,
	title = {Unsupervised video summarization using deep video features},
	number = {画像の認識・理解シンポジウム（MIRU）, PS3-35},
	author = {Otani, Mayu and Nakashima, Yuta and Rahtu, Esa and Heikkilä, Janne and Yokoya, Naokazu},
	month = aug,
	year = {2017},
	pages = {4}
}

@techreport{chenhui_chu__2017,
	title = {画像ピボットパラフレーズ抽出に向けて},
	number = {NLP若手の会 第12回シンポジウム},
	author = {{Chenhui Chu} and {Mayu Otani} and {Yuta Nakashima}},
	month = sep,
	year = {2017},
	pages = {1}
}

@inproceedings{yuta_nakashima_realtime_2017,
	title = {Realtime novel view synthesis with eigen-texture regression},
	abstract = {Realtime novel view synthesis, which generates a novel view of a real object or scene in realtime, enjoys a wide range of applications including augmented reality, telepresence, and immersive telecommunication. Image-based rendering (IBR) with rough geometry can be done using only an off-the-shelf camera and thus can be used by many users. However, IBR from images in the wild (e.g., lighting condition changes or the scene contains objects with specular surfaces) has been a tough problem due to color dis- continuity; IBR with rough geometry picks up appropriate images for a given viewpoint, but the image used for a rendering unit (a face or pixel) switches when the viewpoint moves, which may cause noticeable changes in color. We use the eigen-texture technique, which represents images for a certain face using a point in the eigenspace. We propose to regress a new point in this space, which moves smoothly, given a viewpoint so that we can generate an image whose color smoothly changes according to the point. Our regressor is based on a neural network with a single hidden layer and hyperbolic tangent nonlinearity. We demonstrate the advantages of our IBR approach using our own datasets as well as publicly available datasets for comparison.},
	booktitle = {Proc. {British} {Machine} {Vision} {Conference} ({BMVC})},
	author = {{Yuta Nakashima} and {Fumio Okura} and {Norihiko Kawai} and {Hiroshi Kawasaki} and {Ambrosio Blanco} and {Katsushi Ikeuchi}},
	month = sep,
	year = {2017},
	pages = {12},
	file = {Yuta Nakashima et al. - Realtime novel view synthesis with eigen-texture r.pdf:files/144/Yuta Nakashima et al. - Realtime novel view synthesis with eigen-texture r.pdf:application/pdf}
}

@article{kawai_augmented_2017,
	title = {Augmented reality marker hiding with texture deformation},
	volume = {23},
	issn = {1077-2626},
	url = {https://ieeexplore.ieee.org/document/7593269},
	doi = {10.1109/TVCG.2016.2617325},
	abstract = {Augmented reality (AR) marker hiding is a technique to visually remove AR markers in a real-time video stream. A conventional approach transforms a background image with a homography matrix calculated on the basis of a camera pose and overlays the transformed image on an AR marker region in a real-time frame, assuming that the AR marker is on a planar surface. However, this approach may cause discontinuities in textures around the boundary between the marker and its surrounding area when the planar surface assumption is not satisfied. This paper proposes a method for AR marker hiding without discontinuities around texture boundaries even under nonplanar background geometry without measuring it. For doing this, our method estimates the dense motion in the marker's background by analyzing the motion of sparse feature points around it, together with a smooth motion assumption, and deforms the background image according to it. Our experiments demonstrate the effectiveness of the proposed method in various environments with different background geometries and textures.},
	number = {10},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Kawai, Norihiko and Sato, Tomokazu and Nakashima, Yuta and Yokoya, Naokazu},
	month = oct,
	year = {2017},
	keywords = {AR marker region, augmented reality, augmented reality marker hiding, background geometries, background image, background textures, camera pose, Cameras, diminished reality, Geometry, homography matrix, Image reconstruction, image texture, Marker hiding, matrix algebra, real-time frame, real-time systems, Real-time systems, real-time video stream, sparse feature points, Streaming media, texture deformation, Transforms, Transmission line matrix methods, video streaming},
	pages = {2288--2300},
	file = {IEEE Xplore Abstract Record:files/146/7593269.html:text/html;Kawai et al. - 2017 - Augmented reality marker hiding with texture defor.pdf:files/149/Kawai et al. - 2017 - Augmented reality marker hiding with texture defor.pdf:application/pdf}
}

@techreport{__eigen-texture_2017,
	title = {{自由視点画像生成のためのEigen}-{Texture法における係数の回帰}},
	url = {https://ipsj.ixsq.nii.ac.jp/ej/index.php?active_action=repository_view_main_item_detail&page_id=13&block_id=8&item_id=184114&item_no=1},
	abstract = {物体やシーンの任意の視点における見えを生成する自由視点画像生成の一手法である 3 次元形状情報を利用したイメージベースレンダリングは，通常のカメラで撮影した対象の物体やシーンの画像のみから自由視点画像を生成できることから，一般ユーザでも容易に利用可能である．この手法では，大まかな形状を表す 3 次元メッシュのそれぞれの面に対して撮影方向が最も近い画像を選択し，対応する領域をテクスチャとして貼り付けることで自由視点画像を生成する．このとき，表面の反射などの影響により撮影方向によって物体の色が変化する場合，選択された画像が切り替わる際に色の不連続な変化が生じる．本研究では，3 次元メッシュのある面に貼り付けられるメッシュを固有空間中の点で表現する Eigen-Texture 法を利用し，さらに任意視点の位置から固有空間中の点を回帰することにより，任意の視点移動に対して滑らかにテクスチャが変化する手法を提案し，複数の画像データセットを利用してその有効性を示す．},
	language = {ja},
	number = {情報処理学会 研究報告, CVIM-209-39},
	urldate = {2019-04-30},
	author = {{中島 悠太} and {大倉 史生} and {河合 紀彦} and {川崎 洋} and {池内 克史} and {ブロンコ アンブロージオ}},
	month = nov,
	year = {2017},
	pages = {8},
	file = {Snapshot:files/148/ej.html:text/html}
}

@inproceedings{otani_fine-grained_2017,
	title = {Fine-grained video retrieval for multi-clip video},
	booktitle = {Proc. {Workshop} on {Closing} the {Loop} {Between} {Vision} and {Language} ({CLVL}) at {ICCV}},
	author = {Otani, Mayu and Nakashima, Yuta and Rahtu, Esa and Heikkilä, Janne},
	month = nov,
	year = {2017},
	pages = {2}
}

@article{tejero-de-pablos_summarization_2018,
	title = {Summarization of user-generated sports video by using deep action recognition features},
	volume = {20},
	issn = {1520-9210},
	url = {https://ieeexplore.ieee.org/document/8259321},
	doi = {10.1109/TMM.2018.2794265},
	abstract = {Automatically generating a summary of a sports video poses the challenge of detecting interesting moments, or highlights, of a game. Traditional sports video summarization methods leverage editing conventions of broadcast sports video that facilitate the extraction of high-level semantics. However, user-generated videos are not edited and, thus, traditional methods are not suitable to generate a summary. In order to solve this problem, this paper proposes a novel video summarization method that uses players' actions as a cue to determine the highlights of the original video. A deep neural-network-based approach is used to extract two types of action-related features and to classify video segments into interesting or uninteresting parts. The proposed method can be applied to any sports in which games consist of a succession of actions. Especially, this paper considers the case of Kendo (Japanese fencing) as an example of a sport to evaluate the proposed method. The method is trained using Kendo videos with ground truth labels that indicate the video highlights. The labels are provided by annotators possessing a different experience with respect to Kendo to demonstrate how the proposed method adapts to different needs. The performance of the proposed method is compared with several combinations of different features, and the results show that it outperforms previous summarization methods.},
	number = {8},
	journal = {IEEE Transactions on Multimedia},
	author = {Tejero-de-Pablos, Antonio and Nakashima, Yuta and Sato, Tomokazu and Yokoya, Naokazu and Linna, Marko and Rahtu, Esa},
	month = aug,
	year = {2018},
	keywords = {3D convolutional neural networks, action recognition, action-related features, Cameras, deep action recognition features, deep learning, deep neural-network-based approach, feature extraction, Feature extraction, Games, Hidden Markov models, high-level semantics, image segmentation, interesting parts, Japanese fencing, Kendo videos, long short-term memory, neural nets, player action, Semantics, sport, Sports video summarization, Three-dimensional displays, uninteresting parts, user-generated sport video summarization method, user-generated video, video highlights, video segments, video signal processing},
	pages = {2000--2011},
	annote = {https://arxiv.org/abs/1709.08421
 },
	file = {IEEE Xplore Abstract Record:files/152/8259321.html:text/html;Submitted Version:files/153/Tejero-de-Pablos et al. - 2018 - Summarization of User-Generated Sports Video by Us.pdf:application/pdf}
}

@techreport{otani_finding_2018,
	title = {Finding video parts with natural language},
	url = {https://ipsj.ixsq.nii.ac.jp/ej/index.php?active_action=repository_view_main_item_detail&page_id=13&block_id=8&item_id=186089&item_no=1},
	abstract = {The increasing number of videos have motivated the development of content-based video retrieval (CBVR) methods, which search for videos whose content is relevant to a query. Since most existing datasets for this task provide short video clips capturing a single activity, previous methods have focused on short video clips. However, the majority of real-world videos are more lengthy and edited. Such videos may consist of multiple video clips and may include various content within a video, thus previous methods may fail with real-world videos. In this paper, we propose a new video retrieval task which aims to handle such multi-clip videos. The task is to find query-relevant parts from a video consisting of multiple clips, which we call fine-grained video retrieval (FGVR). For this new task, we build datasets from existing video-description datasets. We synthesize multi-clip video and query pairs by augmenting video-description datasets, which results in large-scale training and evaluation data. We introduce several deep neural network-based approaches as baselines and a training scheme using the synthesized video and query pairs. We investigate the baselines on two datasets built from YouTube and movie datasets, respectively, and present preliminary results.},
	language = {en},
	number = {情報処理学会 研究報告, CVIM-211-7},
	urldate = {2019-04-30},
	author = {Otani, Mayu and Nakashima, Yuta and Rahtu, Esa and Heikkilä, Janne},
	month = feb,
	year = {2018},
	pages = {7},
	file = {Snapshot:files/156/ej.html:text/html}
}

@techreport{chu_extracting_2018,
	title = {Extracting paraphrases grounded by an image},
	url = {https://ipsj.ixsq.nii.ac.jp/ej/index.php?active_action=repository_view_main_item_detail&page_id=13&block_id=8&item_id=186088&item_no=1},
	abstract = {A paraphrase is a restatement of the meaning of a text in other words. Paraphrases have been studied to enhance the performance of many natural language processing tasks. In this paper, we propose a novel task to extract visually grounded paraphrases (VGPs), which are different phrasal expressions describing the same visual concept in an image. These extracted VGPs have the potential to improve language and image multimodal tasks such as visual question answering and image captioning. How to model the similarity between VGPs is the key of VGP extraction. We apply various existing methods as well as propose a novel neural network-based method with image attention, and report the results of the first attempt toward VGP extraction.},
	language = {ja},
	number = {情報処理学会 研究報告, CVIM-211-6},
	urldate = {2019-04-30},
	author = {Chu, Chenhui and Otani, Mayu and Nakashima, Yuta},
	month = feb,
	year = {2018},
	pages = {8},
	file = {Snapshot:files/158/ej.html:text/html}
}

@techreport{chu_visually_2018,
	title = {Visually grounded paraphrase extraction},
	language = {en},
	number = {言語処理学会 第24回年次大会},
	author = {Chu, Chenhui and Otani, Mayu and Nakashima, Yuta},
	month = mar,
	year = {2018},
	pages = {979--982}
}

@techreport{otani_linking_2018,
	title = {Linking videos and languages: {Representations} and their applications},
	url = {https://ipsj.ixsq.nii.ac.jp/ej/index.php?active_action=repository_view_main_item_detail&page_id=13&block_id=8&item_id=187500&item_no=1},
	abstract = {Mimicking the human ability to understand visual data (images or videos) is a long-standing goal of computer vision. To achieve visual content understanding in a computer, many recent works attempt to connect visual and natural language data including object labels and descriptions. This attempt is important not only for visual understanding but also for broad applications such as content-based visual data retrieval and automatic description generation to help visually impaired people. The goal of this paper is to develop cross-modal representations, which enable us to associate videos with natural language. We explorer two directions for constructing cross-modal representations: hand-crafted representations and data-driven representation learning. The experiments demonstrate the proposed representations can be applied to a wide range of practical applications including query-focused video summarization and content-based video retrieval with natural language queries.},
	language = {en},
	number = {情報処理学会 研究報告, CVIM-212-38},
	urldate = {2019-04-30},
	author = {Otani, Mayu and Nakashima, Yuta and Rahtu, Esa and Heikkilä, Janne and Yokoya, Naokazu},
	month = may,
	year = {2018},
	pages = {16},
	file = {Snapshot:files/161/ej.html:text/html}
}

@inproceedings{mayu_otani_visually_2018,
	title = {Visually grounded paraphrase extraction via phrase grounding},
	language = {en},
	booktitle = {Proc. {Workshop} on {Language} and {Vision} at {CVPR}},
	author = {{Mayu Otani} and {Chenhui Chu} and {Yuta Nakashima}},
	month = jun,
	year = {2018},
	pages = {3}
}

@inproceedings{chu_iparaphrasing:_2018,
	title = {{iParaphrasing}: {Extracting} visually grounded paraphrases via an image},
	shorttitle = {{iParaphrasing}},
	url = {https://aclweb.org/anthology/papers/C/C18/C18-1295/},
	language = {en-us},
	urldate = {2019-04-30},
	author = {Chu, Chenhui and Otani, Mayu and Nakashima, Yuta},
	month = aug,
	year = {2018},
	pages = {3479--3492},
	annote = {https://arxiv.org/abs/1806.04284
 },
	file = {Full Text PDF:files/164/Chu et al. - 2018 - iParaphrasing Extracting Visually Grounded Paraph.pdf:application/pdf;Snapshot:files/165/C18-1295.html:text/html}
}

@article{tanaka_iterative_2018,
	title = {Iterative applications of image completion with {CNN}-based failure detection},
	volume = {55},
	issn = {1047-3203},
	url = {http://www.sciencedirect.com/science/article/pii/S104732031830110X},
	doi = {10.1016/j.jvcir.2018.05.015},
	abstract = {Image completion is a technique to fill missing regions in a damaged or redacted image. A patch-based approach is one of major approaches, which solves an optimization problem that involves pixel values in missing regions and similar image patch search. One major problem of this approach is that it sometimes duplicates implausible texture in the image or overly smooths down a missing region when the algorithm cannot find better patches. As a practical remedy, the user may provide an interaction to identify such regions and re-apply image completion iteratively until she/he gets a desirable result. In this work, inspired by this idea, we propose a framework of human-in-the-loop style image completion with automatic failure detection using a deep neural network instead of human interaction. Our neural network takes small patches extracted from multiple feature maps obtained from the completion process as input for the automated interaction process, which is iterated several times. We experimentally show that our neural network outperforms a conventional linear support vector machine. Our subjective evaluation demonstrates that our method drastically improves the visual quality of resulting images compared to non-iterative application.},
	urldate = {2019-04-30},
	journal = {Journal of Visual Communication and Image Representation},
	author = {Tanaka, Takahiro and Kawai, Norihiko and Nakashima, Yuta and Sato, Tomokazu and Yokoya, Naokazu},
	month = aug,
	year = {2018},
	keywords = {Convolutional neural network, Failure detection, Image completion, Image inpainting},
	pages = {56--66},
	file = {ScienceDirect Snapshot:files/168/S104732031830110X.html:text/html}
}

@techreport{benjamin_renoust_exploration_2018,
	title = {Exploration and mining of 50,000 {Buddha} pictures},
	number = {画像の認識・理解シンポジウム（MIRU）},
	author = {{Benjamin Renoust} and {Ayaka Uesaka} and {Yuta Nakashima} and {Hajime Nagahara} and {Yutaka Fujioka}},
	month = aug,
	year = {2018},
	pages = {4}
}

@techreport{mayu_otani_phrase_2018,
	title = {Phrase localization-based visually grounded paraphrase identification},
	number = {画像の認識・理解シンポジウム（MIRU）},
	author = {{Mayu Otani} and {Chenhui Chu} and {Yuta Nakashima}},
	month = aug,
	year = {2018},
	pages = {4}
}

@techreport{akihiko_sayo_synthesis_2018,
	title = {Synthesis of human shape in loose cloth using eigen-deformation},
	number = {画像の認識・理解シンポジウム（MIRU）},
	author = {{Akihiko Sayo} and {Ryosuke Kimura} and {Fabian Lorenzo Dayrit} and {Yuta Nakashima} and {Hiroshi Kawasaki} and {Ambrosio Blanco} and {Katsushi Ikeuchi}},
	month = aug,
	year = {2018},
	pages = {4}
}

@inproceedings{kimura_representing_2018,
	title = {Representing a partially observed non-rigid 3D human using eigen-texture and eigen-deformation},
	url = {https://ieeexplore.ieee.org/document/8545658},
	doi = {10.1109/ICPR.2018.8545658},
	abstract = {Reconstruction of the shape and motion of humans from RGB-D is a challenging problem, receiving much attention in recent years. Recent approaches for full-body reconstruction use a statistic shape model, which is built upon accurate full-body scans of people in skin-tight clothes, to complete invisible parts due to occlusion. Such a statistic model may still be fit to an RGB-D measurement with loose clothes but cannot describe its deformations, such as clothing wrinkles. Observed surfaces may be reconstructed precisely from actual measurements, while we have no cues for unobserved surfaces. For full-body reconstruction with loose clothes, we propose to use lower dimensional embeddings of texture and deformation referred to as eigen-texturing and eigen-deformation, to reproduce views of even unobserved surfaces. Provided a full-body reconstruction from a sequence of partial measurements as 3D meshes, the texture and deformation of each triangle are then embedded using eigen-decomposition. Combined with neural-network-based coefficient regression, our method synthesizes the texture and deformation from arbitrary viewpoints. We evaluate our method using simulated data and visually demonstrate how our method works on real data.},
	booktitle = {Proc. 24th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Kimura, Ryosuke and Sayo, Akihiko and Dayrit, Fabian Lorenzo and Nakashima, Yuta and Kawasaki, Hiroshi and Blanco, Ambrosio and Ikeuchi, Katsushi},
	month = aug,
	year = {2018},
	keywords = {clothing, Clothing, clothing wrinkles, computer animation, Data models, Deformable models, deformations, eigen-deformation, eigen-texture, eigendecomposition, eigendeformation, eigentexturing, full-body reconstruction, human motion capture, image colour analysis, image motion analysis, image reconstruction, image texture, loose clothes, medical image processing, neural nets, Non-rigid 3D deformation, partial measurements, partially observed nonrigid 3D human, regression analysis, RGB-D measurement, Shape, skin-tight clothes, solid modelling, statistic model, statistic shape model, statistical analysis, Strain, Surface reconstruction, Three-dimensional displays},
	pages = {1043--1048},
	annote = {https://arxiv.org/abs/1807.02632},
	file = {IEEE Xplore Abstract Record:files/178/8545658.html:text/html;Submitted Version:files/179/Kimura et al. - 2018 - Representing a Partially Observed Non-Rigid 3D Hum.pdf:application/pdf}
}

@article{otani_finding_2018-1,
	title = {Finding important people in a video using deep neural networks with conditional random fields},
	volume = {E101.D},
	issn = {0916-8532, 1745-1361},
	url = {https://www.jstage.jst.go.jp/article/transinf/E101.D/10/E101.D_2018EDP7029/_article/-char/ja},
	doi = {10.1587/transinf.2018EDP7029},
	abstract = {Finding important regions is essential for applications, such as content-aware video compression and video retargeting to automatically crop a region in a video for small screens. Since people are one of main subjects when taking a video, some methods for finding important regions use a visual attention model based on face/pedestrian detection to incorporate the knowledge that people are important. However, such methods usually do not distinguish important people from passers-by and bystanders, which results in false positives. In this paper, we propose a deep neural network (DNN)-based method, which classifies a person into important or unimportant, given a video containing multiple people in a single frame and captured with a hand-held camera. Intuitively, important/unimportant labels are highly correlated given that corresponding people's spatial motions are similar. Based on this assumption, we propose to boost the performance of our important/unimportant classification by using conditional random fields (CRFs) built upon the DNN, which can be trained in an end-to-end manner. Our experimental results show that our method successfully classifies important people and the use of a DNN with CRFs improves the accuracy.},
	language = {en},
	number = {10},
	urldate = {2019-04-30},
	journal = {IEICE Transactions on Information and Systems},
	author = {Otani, Mayu and Nishida, Atsushi and Nakashima, Yuta and Sato, Tomokazu and Yokoya, Naokazu},
	month = oct,
	year = {2018},
	pages = {2509--2517},
	file = {Full Text PDF:files/182/Otani et al. - 2018 - Finding Important People in a Video Using Deep Neu.pdf:application/pdf;Snapshot:files/183/ja.html:text/html}
}

@techreport{__3_2019,
	title = {多重焦点顕微鏡画像列からの細胞の3次元形状復元},
	url = {https://ipsj.ixsq.nii.ac.jp/ej/index.php?active_action=repository_view_main_item_detail&page_id=13&block_id=8&item_id=193858&item_no=1},
	abstract = {本研究では，焦点を変えながら撮影した細胞の多重焦点顕微鏡画像列から，細胞の 3 次元形状を推定する手法を提案する．まず，細胞内の物質によって透過光が減衰し，それが顕微鏡のレンズにより集光されることで顕微鏡画像が得られる，という光学モデルを仮定し，この光学モデルに基づいた画像生成を定式化する．この式により推定される輝度値と，実際の画像の輝度値の誤差を最小化することで，細胞の 3 次元透過率分布を推定し，それにより形状復元を行う．シミュレーションでは，細胞の CG モデルを使って多重焦点画像列を人工的に作成し，提案手法より推定した透過率と，真値を比較することで，本手法の有効性を確認した．また，実際の細胞の画像から 3 次元形状復元を行った．},
	language = {ja},
	number = {情報処理学会研究報告, CVIM-215-33},
	urldate = {2019-04-30},
	author = {{山口 貴大} and {長原 一} and {諸岡 健一} and {中島 悠太} and {浦西 友樹} and {倉爪 亮} and {大野 英治}},
	month = jan,
	year = {2019},
	pages = {7},
	file = {Snapshot:files/185/index.html:text/html}
}

@inproceedings{shizuka_shirai_multimodal_2019,
	title = {Multimodal learning analytics: {Society} 5.0 project in {Japan}},
	booktitle = {Proc. 9th {International} {Conference} on {Learning} {Analytics} and {Knowledge} ({LAK})},
	author = {{Shizuka Shirai} and {Noriko Takemura} and {Yuta Nakashima} and {Hajime Nagahara} and {Haruo Takemura}},
	month = mar,
	year = {2019},
	pages = {4}
}

@inproceedings{otani_rethinking_2019,
	title = {Rethinking the evaluation of video summaries},
	url = {http://arxiv.org/abs/1903.11328},
	abstract = {Video summarization is a technique to create a short skim of the original video while preserving the main stories/content. There exists a substantial interest in automatizing this process due to the rapid growth of the available material. The recent progress has been facilitated by public benchmark datasets, which enable easy and fair comparison of methods. Currently the established evaluation protocol is to compare the generated summary with respect to a set of reference summaries provided by the dataset. In this paper, we will provide in-depth assessment of this pipeline using two popular benchmark datasets. Surprisingly, we observe that randomly generated summaries achieve comparable or better performance to the state-of-the-art. In some cases, the random summaries outperform even the human generated summaries in leave-one-out experiments. Moreover, it turns out that the video segmentation, which is often considered as a fixed pre-processing method, has the most significant impact on the performance measure. Based on our observations, we propose alternative approaches for assessing the importance scores as well as an intuitive visualization of correlation between the estimated scoring and human annotations.},
	urldate = {2019-04-30},
	booktitle = {{arXiv}:1903.11328 [cs]},
	author = {Otani, Mayu and Nakashima, Yuta and Rahtu, Esa and Heikkilä, Janne},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.11328},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR'19 poster},
	file = {arXiv\:1903.11328 PDF:files/189/Otani et al. - 2019 - Rethinking the Evaluation of Video Summaries.pdf:application/pdf;arXiv.org Snapshot:files/190/1903.html:text/html}
}

@article{garcia_context-aware_2019,
	title = {Context-aware embeddings for automatic art analysis},
	url = {http://arxiv.org/abs/1904.04985},
	doi = {10.1145/3323873.3325028},
	abstract = {Automatic art analysis aims to classify and retrieve artistic representations from a collection of images by using computer vision and machine learning techniques. In this work, we propose to enhance visual representations from neural networks with contextual artistic information. Whereas visual representations are able to capture information about the content and the style of an artwork, our proposed context-aware embeddings additionally encode relationships between different artistic attributes, such as author, school, or historical period. We design two different approaches for using context in automatic art analysis. In the first one, contextual data is obtained through a multi-task learning model, in which several attributes are trained together to find visual relationships between elements. In the second approach, context is obtained through an art-specific knowledge graph, which encodes relationships between artistic attributes. An exhaustive evaluation of both of our models in several art analysis problems, such as author identification, type classification, or cross-modal retrieval, show that performance is improved by up to 7.3\% in art classification and 37.24\% in retrieval when context-aware embeddings are used.},
	urldate = {2019-04-30},
	journal = {arXiv:1904.04985 [cs]},
	author = {Garcia, Noa and Renoust, Benjamin and Nakashima, Yuta},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.04985},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1904.04985 PDF:files/192/Garcia et al. - 2019 - Context-Aware Embeddings for Automatic Art Analysi.pdf:application/pdf;arXiv.org Snapshot:files/193/1904.html:text/html}
}