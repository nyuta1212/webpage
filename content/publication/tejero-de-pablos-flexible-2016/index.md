+++
title = "Flexible human action recognition in depth video sequences using masked joint trajectories"
date = 2016-06-01
authors = ["Antonio Tejero-de-Pablos", "Yuta Nakashima", "Naokazu Yokoya", "Francisco-Javier Díaz-Pernas", "Mario Martínez-Zarzuela"]
publication_types = ["2"]
abstract = "Human action recognition applications are greatly benefited from the use of commodity depth sensors that are capable of skeleton tracking. Some of these applications (e.g., customizable gesture interfaces) require learning of new actions at runtime and may not count with many training instances. This paper presents a human action recognition method designed for flexibility, which allows taking users’ feedback to improve recognition performance and to add a new action instance without computationally expensive optimization for training classifiers. Our nearest neighbor-based action classifier adopts dynamic time warping to handle variability in execution rate. In addition, it uses the confidence values associated to each tracked joint position to mask erroneous trajectories for robustness against noise. We evaluate the proposed method with various datasets with different frame rates, actors, and noise. The experimental results demonstrate its adequacy for learning of actions from depth sequences at runtime. We achieve an accuracy comparable to the state-of-the-art techniques on the challenging MSR-Action3D dataset."
featured = false
publication = "*EURASIP Journal on Image and Video Processing*"
url_pdf = "https://doi.org/10.1186/s13640-016-0120-y"
doi = "10.1186/s13640-016-0120-y"
+++

