+++
title = "Video summarization using deep semantic features"
date = 2016-09-01
authors = ["Mayu Otani", "Yuta Nakashima", "Esa Rahtu", "Janne Heikkil√§", "Naokazu Yokoya"]
publication_types = ["1"]
abstract = "This paper presents a video summarization technique for an Internet video to provide a quick way to overview its content. This is a challenging problem because finding important or informative parts of the original video requires to understand its content. Furthermore the content of Internet videos is very diverse, ranging from home videos to documentaries, which makes video summarization much more tough as prior knowledge is almost not available. To tackle this problem, we propose to use deep video features that can encode various levels of content semantics, including objects, actions, and scenes, improving the efficiency of standard video summarization techniques. For this, we design a deep neural network that maps videos as well as descriptions to a common semantic space and jointly trained it with associated pairs of videos and descriptions. To generate a video summary, we extract the deep features from each segment of the original video and apply a clustering-based summarization technique to them. We evaluate our video summaries using the SumMe dataset as well as baseline approaches. The results demonstrated the advantages of incorporating our deep semantic features in a video summarization technique."
featured = false
publication = "*Proc. 13th Asian Conference on Computer Vision (ACCV)*"
tags = ["Computer Science - Computer Vision and Pattern Recognition"]
url_pdf = "http://arxiv.org/abs/1609.08758"
+++

