@techreport{otani_linking_2018,
 abstract = {Mimicking the human ability to understand visual data (images or videos) is a long-standing goal of computer vision. To achieve visual content understanding in a computer, many recent works attempt to connect visual and natural language data including object labels and descriptions. This attempt is important not only for visual understanding but also for broad applications such as content-based visual data retrieval and automatic description generation to help visually impaired people. The goal of this paper is to develop cross-modal representations, which enable us to associate videos with natural language. We explorer two directions for constructing cross-modal representations: hand-crafted representations and data-driven representation learning. The experiments demonstrate the proposed representations can be applied to a wide range of practical applications including query-focused video summarization and content-based video retrieval with natural language queries.},
 author = {Otani, Mayu and Nakashima, Yuta and Rahtu, Esa and Heikkilä, Janne and Yokoya, Naokazu},
 file = {Snapshot:/Users/n-yuta/Zotero/storage/P4TYJNGD/ej.html:text/html},
 language = {en},
 month = {May},
 number = {情報処理学会 研究報告, CVIM-212-38},
 pages = {16},
 title = {Linking videos and languages: Representations and their applications},
 url = {https://ipsj.ixsq.nii.ac.jp/ej/index.php?active_action=repository_view_main_item_detail&page_id=13&block_id=8&item_id=187500&item_no=1},
 urldate = {2019-04-30},
 year = {2018}
}

